{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from google import genai\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import pathlib\n",
    "from google.genai import types\n",
    "import fitz  # PyMuPDF\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    \"gemini-2.0-flash\",\n",
    "    \"gemini-2.0-pro-exp-02-05\",\n",
    "    \"gemini-2.5-pro-exp-03-25\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from load_dotenv import load_dotenv\n",
    "\n",
    "# load_dotenv()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = os.environ.get(\"GEMINI_API_KEY\")\n",
    "client = genai.Client(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf2 = \"Capacity-Aware_Inference_Mitigating_the_Straggler_Effect_in_Mixture_of_Experts.pdf\"\n",
    "pdf1 = \"Mixture_of_Modular_Experts_Distilling_Knowledge_from_a_Multilingual_Teacher_into_Specialized_Modular_Language_Models.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf1_filepath = pathlib.Path(pdf1)\n",
    "pdf2_filepath = pathlib.Path(pdf2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's Call Gemini!\n",
    "- Practice Calling the Gemini API:\n",
    "    - Single Call\n",
    "    - Chat\n",
    "    - PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alright, let's break down how AI works.  It's a broad topic, so I'll give you a general overview, then we can dive deeper into specific areas if you're interested.\n",
      "\n",
      "**The Core Idea:**\n",
      "\n",
      "At its heart, AI is about creating machines that can perform tasks that typically require human intelligence.  These tasks include:\n",
      "\n",
      "*   **Learning:** Acquiring information and rules for using the information.\n",
      "*   **Reasoning:** Using rules to reach conclusions (both exact and approximate).\n",
      "*   **Problem Solving:** Identifying and solving problems.\n",
      "*   **Perception:** Recognizing objects, sounds, speech, and other inputs.\n",
      "*   **Understanding Natural Language:** Processing and understanding human language.\n",
      "\n",
      "**Key Components & Approaches:**\n",
      "\n",
      "AI achieves these feats through a variety of techniques, but here are some of the most fundamental:\n",
      "\n",
      "1.  **Algorithms:**\n",
      "\n",
      "    *   These are the step-by-step instructions that tell the computer how to perform a task. They are the backbone of AI systems. Think of them as recipes.\n",
      "\n",
      "2.  **Data:**\n",
      "\n",
      "    *   AI thrives on data. The more data an AI system has, the better it can learn and improve.  This data can be in the form of text, images, audio, video, or any other structured information. Data acts as the ingredients for AI systems.\n",
      "\n",
      "3.  **Machine Learning (ML):**\n",
      "\n",
      "    *   This is a subset of AI where systems *learn from data without being explicitly programmed*.  Instead of someone writing rules for every possible scenario, the algorithm identifies patterns and makes predictions based on the data it's trained on.\n",
      "    *   **Types of Machine Learning:**\n",
      "        *   **Supervised Learning:** The algorithm is trained on a labeled dataset (e.g., images labeled as \"cat\" or \"dog\"). It learns to predict the correct label for new, unseen data.\n",
      "        *   **Unsupervised Learning:** The algorithm is trained on an unlabeled dataset and tries to find patterns or structures within the data (e.g., clustering customers into different groups based on their purchasing behavior).\n",
      "        *   **Reinforcement Learning:** The algorithm learns by interacting with an environment and receiving rewards or penalties for its actions. It tries to learn the optimal strategy to maximize its rewards (e.g., training a robot to navigate a maze).\n",
      "\n",
      "4.  **Deep Learning (DL):**\n",
      "\n",
      "    *   A subfield of machine learning that uses artificial neural networks with many layers (hence \"deep\"). These networks are inspired by the structure of the human brain.\n",
      "    *   **Neural Networks:**  These are complex interconnected networks of nodes (artificial neurons) that process information.  Each connection between neurons has a weight associated with it, which is adjusted during the learning process. The more layers, the more complex the relationship the network can discover.\n",
      "    *   Deep learning is particularly effective for tasks like image recognition, natural language processing, and speech recognition.\n",
      "\n",
      "5.  **Natural Language Processing (NLP):**\n",
      "\n",
      "    *   Focuses on enabling computers to understand, interpret, and generate human language.\n",
      "    *   **Techniques used in NLP:**\n",
      "        *   **Tokenization:** Breaking down text into individual words or units.\n",
      "        *   **Parsing:** Analyzing the grammatical structure of sentences.\n",
      "        *   **Sentiment Analysis:** Determining the emotional tone of text.\n",
      "        *   **Machine Translation:** Automatically translating text from one language to another.\n",
      "\n",
      "6.  **Computer Vision:**\n",
      "\n",
      "    *   Enables computers to \"see\" and interpret images and videos.\n",
      "    *   **Techniques used in Computer Vision:**\n",
      "        *   **Image Recognition:** Identifying objects in an image.\n",
      "        *   **Object Detection:** Locating and identifying multiple objects in an image.\n",
      "        *   **Image Segmentation:** Dividing an image into different regions.\n",
      "\n",
      "7.  **Rule-Based Systems (Expert Systems):**\n",
      "\n",
      "    *   Older approach to AI where knowledge is encoded in a set of \"if-then\" rules.\n",
      "    *   These systems are good for tasks where the rules are well-defined and the domain is limited.\n",
      "    *   Example: A medical diagnosis system that asks a series of questions and uses rules to determine the most likely diagnosis.\n",
      "\n",
      "**The Learning Process (Simplified):**\n",
      "\n",
      "1.  **Data Collection:** Gather a large dataset relevant to the task.\n",
      "2.  **Data Preparation:** Clean and preprocess the data.  This often involves removing errors, handling missing values, and transforming the data into a suitable format.\n",
      "3.  **Model Selection:** Choose an appropriate AI model or algorithm (e.g., a neural network, a decision tree, a support vector machine).\n",
      "4.  **Training:** Feed the data into the model, and the model adjusts its internal parameters to learn the patterns and relationships in the data.  This is often an iterative process that involves repeatedly showing the model the data and refining its parameters until it achieves a desired level of accuracy.\n",
      "5.  **Evaluation:** Test the model on a separate dataset to evaluate its performance and ensure it generalizes well to unseen data.\n",
      "6.  **Deployment:** Deploy the trained model into a real-world application.\n",
      "7.  **Monitoring and Retraining:** Continuously monitor the model's performance and retrain it with new data to maintain its accuracy and relevance.\n",
      "\n",
      "**Example: Image Recognition (Classifying Images as Cats or Dogs):**\n",
      "\n",
      "1.  **Data:** A large dataset of images, each labeled as either \"cat\" or \"dog.\"\n",
      "2.  **Model:** A convolutional neural network (CNN), a type of deep learning model well-suited for image recognition.\n",
      "3.  **Training:** The CNN is fed the images, and it learns to identify features that are characteristic of cats and dogs (e.g., pointy ears, whiskers, snout shape).\n",
      "4.  **Prediction:** When presented with a new image, the CNN analyzes its features and predicts whether it's a cat or a dog.\n",
      "\n",
      "**Levels of AI:**\n",
      "\n",
      "*   **Narrow or Weak AI:** Designed to perform a specific task (e.g., playing chess, recognizing faces).  Most AI systems today fall into this category.\n",
      "*   **General or Strong AI:** Possesses human-level intelligence and can perform any intellectual task that a human being can.  This is still largely theoretical.\n",
      "*   **Super AI:** Surpasses human intelligence in all aspects. Also theoretical and raises ethical concerns.\n",
      "\n",
      "**Key Takeaways:**\n",
      "\n",
      "*   AI is about making machines that can perform tasks requiring intelligence.\n",
      "*   It uses algorithms, data, and various techniques like machine learning, deep learning, NLP, and computer vision.\n",
      "*   Machine learning allows systems to learn from data without explicit programming.\n",
      "*   Deep learning utilizes artificial neural networks with multiple layers.\n",
      "*   AI is constantly evolving, with new techniques and applications emerging all the time.\n",
      "\n",
      "**Where do you want to go next?  We can delve into:**\n",
      "\n",
      "*   A specific type of machine learning algorithm (e.g., neural networks, decision trees).\n",
      "*   A particular application of AI (e.g., self-driving cars, medical diagnosis, fraud detection).\n",
      "*   The ethical considerations of AI.\n",
      "*   The history of AI.\n",
      "*   The programming languages used in AI (e.g. Python, R).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=models[1],\n",
    "    contents=[\"How does AI work?\"]\n",
    ")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, let's dive into the world of OCR: Optical Character Recognition.\n",
      "\n",
      "**What is OCR?**\n",
      "\n",
      "At its core, Optical Character Recognition (OCR) is a technology that allows computers to \"read\" text from images, scanned documents, or even handwriting.  It essentially converts images of text into machine-readable text data. Think of it as giving computers the ability to understand written language in a visual format.\n",
      "\n",
      "**How does it work?**\n",
      "\n",
      "The OCR process typically involves these key steps:\n",
      "\n",
      "1.  **Image Acquisition:** The process starts with an image containing text. This could be a scanned document, a photo taken with a camera, or even an image from a PDF file.\n",
      "\n",
      "2.  **Preprocessing:** This stage enhances the image to improve OCR accuracy. Common preprocessing steps include:\n",
      "    *   **Deskewing:** Correcting any slant or rotation in the image.\n",
      "    *   **Noise Reduction:** Removing unwanted specks, shadows, or other imperfections.\n",
      "    *   **Binarization:** Converting the image to black and white, making it easier to distinguish characters from the background.\n",
      "    *   **Line Removal/Correction:** Isolating the lines of text and attempting to straighten warped lines.\n",
      "\n",
      "3.  **Character Segmentation:** This involves identifying and isolating individual characters (letters, numbers, symbols) within the image. This can be a challenging step, especially with connected or poorly formed characters.\n",
      "\n",
      "4.  **Character Recognition:** This is where the magic happens. The OCR engine compares each isolated character to a database of known character shapes (fonts and styles). It uses various techniques, including:\n",
      "    *   **Pattern Matching:** Comparing the character to a pre-defined set of character templates.\n",
      "    *   **Feature Extraction:** Identifying key features of the character (e.g., lines, curves, loops) and using those features to classify the character.\n",
      "    *   **Machine Learning (Deep Learning):** Modern OCR systems increasingly rely on machine learning models, especially Convolutional Neural Networks (CNNs), which are trained on vast datasets of images and text to recognize characters with high accuracy. Deep learning models can also handle variations in fonts, styles, and image quality.\n",
      "\n",
      "5.  **Post-processing:** This stage cleans up the recognized text, correcting errors and improving readability. This might involve:\n",
      "    *   **Spell Checking:** Identifying and correcting misspelled words.\n",
      "    *   **Contextual Analysis:** Using the surrounding words to determine the most likely correct character, especially when the OCR engine is unsure.\n",
      "    *   **Formatting Restoration:** Attempting to reconstruct the original layout and formatting of the document (e.g., headings, paragraphs, tables).\n",
      "\n",
      "**Uses and Applications of OCR:**\n",
      "\n",
      "OCR has a wide range of applications across various industries:\n",
      "\n",
      "*   **Document Management:** Converting scanned documents into searchable and editable files, reducing paper clutter, and improving accessibility.\n",
      "*   **Data Entry Automation:** Extracting data from forms, invoices, receipts, and other documents, automating data entry processes, and reducing manual labor.\n",
      "*   **Banking and Finance:** Processing checks, loan applications, and other financial documents.\n",
      "*   **Healthcare:** Extracting information from patient records, medical reports, and insurance claims.\n",
      "*   **Legal:** Reviewing and analyzing legal documents, contracts, and evidence.\n",
      "*   **Library and Archival:** Digitizing books, newspapers, and other historical documents, preserving them for future generations.\n",
      "*   **Accessibility:** Making printed materials accessible to people with disabilities by converting them to text-to-speech or braille formats.\n",
      "*   **Mobile Apps:** Capturing text from signs, menus, and other real-world objects using smartphone cameras. Translation apps use OCR to read foreign text and translate it.\n",
      "*   **License Plate Recognition (LPR):** Used in parking systems, toll booths, and law enforcement to automatically identify vehicle license plates.\n",
      "*   **Robotics and Automation:** Enabling robots to \"read\" labels, instructions, and other information in their environment.\n",
      "*   **Invoice Processing:** Automating the extraction of key data (supplier name, invoice number, amounts, etc.) from invoices.\n",
      "*   **Automated Form Processing:** Extracting data from structured or semi-structured forms.\n",
      "\n",
      "**Advantages of OCR:**\n",
      "\n",
      "*   **Increased Efficiency:** Automates tasks that would otherwise require manual data entry, saving time and reducing errors.\n",
      "*   **Improved Accuracy:** Modern OCR systems can achieve very high accuracy rates, especially with clean, well-formatted documents.\n",
      "*   **Reduced Costs:** Reduces labor costs associated with manual data entry and document processing.\n",
      "*   **Enhanced Accessibility:** Makes printed materials accessible to people with disabilities.\n",
      "*   **Better Searchability:** Allows you to search and retrieve information from scanned documents.\n",
      "*   **Space Saving:** Reduces the need for physical storage of paper documents.\n",
      "\n",
      "**Limitations of OCR:**\n",
      "\n",
      "*   **Accuracy Issues:** OCR accuracy can be affected by poor image quality, complex layouts, unusual fonts, handwriting, and damage to the original document.\n",
      "*   **Font and Language Support:** Some OCR engines may not support all fonts and languages.\n",
      "*   **Cost:** Commercial OCR software can be expensive.\n",
      "*   **Complexity:** Setting up and configuring OCR systems can be complex, especially for large-scale applications.\n",
      "*   **Handwriting Recognition:** While OCR has improved, handwriting recognition is still challenging and less accurate than printed text recognition.  The variability of handwriting styles makes it difficult to achieve high accuracy.\n",
      "*   **Table Extraction:** Extracting data from tables can be difficult, especially with complex or irregular table structures.\n",
      "\n",
      "**Types of OCR Software and Engines:**\n",
      "\n",
      "*   **Commercial OCR Software:** ABBYY FineReader, Readiris, OmniPage.  These are often desktop applications with advanced features and high accuracy.\n",
      "*   **Online OCR Services:** Google Cloud Vision API, Microsoft Azure Computer Vision API, Amazon Textract. These are cloud-based services that offer OCR as an API, allowing developers to integrate OCR functionality into their applications.  They often operate on a pay-per-use basis.\n",
      "*   **Open-Source OCR Engines:** Tesseract OCR (one of the most popular open-source engines, backed by Google), OCRopus.\n",
      "*   **OCR Libraries and Toolkits:** Developers can use OCR libraries and toolkits to build custom OCR applications.\n",
      "\n",
      "**Factors Affecting OCR Accuracy:**\n",
      "\n",
      "*   **Image Quality:** High-resolution, clear images with good contrast are essential for accurate OCR.\n",
      "*   **Font Type and Size:** Simple, common fonts are easier to recognize than complex or unusual fonts.  Larger font sizes generally lead to better accuracy.\n",
      "*   **Document Layout:** Simple, well-structured layouts are easier to process than complex layouts with multiple columns, tables, and images.\n",
      "*   **Language:** The OCR engine must be trained to recognize the language of the text.\n",
      "*   **Noise and Distortion:** Noise, distortion, and other imperfections in the image can negatively affect OCR accuracy.\n",
      "*   **Skew:** Skewed images (images that are rotated) can cause errors in OCR.  Deskewing is an important preprocessing step.\n",
      "\n",
      "**Key Takeaways:**\n",
      "\n",
      "*   OCR is a powerful technology that converts images of text into machine-readable text.\n",
      "*   It has a wide range of applications in various industries.\n",
      "*   OCR accuracy depends on factors such as image quality, font, and document layout.\n",
      "*   There are various types of OCR software and engines available, from commercial desktop applications to cloud-based APIs and open-source options.\n",
      "*   Modern OCR is increasingly powered by Machine Learning/Deep Learning, significantly improving the accuracy of character recognition.\n",
      "\n",
      "If you have any more specific questions about OCR, feel free to ask! For example, you might want to know about specific OCR software, handwriting recognition, or OCR APIs.  Let me know what interests you!\n",
      "\n",
      "Okay, let's break down the usefulness of OCR into concrete examples, highlighting both personal and professional applications:\n",
      "\n",
      "**1. Saving Time and Increasing Efficiency:**\n",
      "\n",
      "*   **The Problem:** Manually typing information from documents is incredibly time-consuming and prone to errors. Imagine needing to extract all the invoice numbers from a stack of 100 invoices.\n",
      "*   **OCR Solution:** OCR software can scan those invoices and automatically extract the invoice numbers into a spreadsheet in minutes. This saves countless hours of manual data entry and eliminates potential typos.\n",
      "*   **Real-World Example:** An accounting department processing hundreds of invoices daily uses OCR to automate data entry, freeing up staff to focus on more strategic tasks.\n",
      "\n",
      "**2. Enhanced Document Management and Searchability:**\n",
      "\n",
      "*   **The Problem:** Paper documents are difficult to search and organize. Finding a specific piece of information within a large archive of paper files can be a nightmare.\n",
      "*   **OCR Solution:** OCR converts scanned documents into searchable PDFs.  This means you can search for specific keywords or phrases within the document, even though it originated as an image.\n",
      "*   **Real-World Example:** A law firm digitizes all its legal documents using OCR, allowing lawyers to quickly find relevant case files and precedents by searching for keywords.\n",
      "\n",
      "**3. Automating Data Entry and Business Processes:**\n",
      "\n",
      "*   **The Problem:** Manual data entry is a bottleneck in many business processes, such as processing orders, handling customer inquiries, and managing inventory.\n",
      "*   **OCR Solution:** OCR can be integrated into business workflows to automate data extraction from forms, invoices, receipts, and other documents. This data can then be automatically fed into databases, CRM systems, and other business applications.\n",
      "*   **Real-World Example:** An e-commerce company uses OCR to automatically process order forms received via email, reducing order processing time and improving customer satisfaction.\n",
      "\n",
      "**4. Making Information Accessible to People with Disabilities:**\n",
      "\n",
      "*   **The Problem:** Printed materials are inaccessible to people with visual impairments or other disabilities.\n",
      "*   **OCR Solution:** OCR can convert printed text into digital text that can be read aloud by screen readers or displayed in braille, making information accessible to a wider audience.\n",
      "*   **Real-World Example:** A university uses OCR to convert textbooks and course materials into accessible formats for students with visual impairments.\n",
      "\n",
      "**5. Mobile Productivity and Convenience:**\n",
      "\n",
      "*   **The Problem:**  Copying text from printed materials or images on your phone can be tedious.\n",
      "*   **OCR Solution:**  Mobile OCR apps allow you to take a picture of text (e.g., a sign, a restaurant menu) and instantly convert it into editable text on your phone. This makes it easy to copy and paste text into emails, notes, or other applications.\n",
      "*   **Real-World Example:** A traveler uses a mobile OCR app to translate foreign language menus or signs while abroad.\n",
      "\n",
      "**6. Improved Customer Service:**\n",
      "\n",
      "*   **The Problem:**  Customers may submit forms, documents, or images containing important information. Manually processing these submissions can be slow and inefficient.\n",
      "*   **OCR Solution:**  OCR can automatically extract information from these submissions, allowing customer service representatives to quickly access and process customer requests.\n",
      "*   **Real-World Example:**  An insurance company uses OCR to process claims forms submitted by customers, speeding up the claims process and improving customer satisfaction.\n",
      "\n",
      "**7. Digitizing and Preserving Historical Documents:**\n",
      "\n",
      "*   **The Problem:**  Old and fragile documents are at risk of deterioration and loss.\n",
      "*   **OCR Solution:**  OCR allows libraries and archives to digitize historical documents, preserving them for future generations.\n",
      "*   **Real-World Example:**  A historical society digitizes its collection of old letters and manuscripts using OCR, making them accessible to researchers and the public online.\n",
      "\n",
      "**8. Cost Reduction:**\n",
      "\n",
      "*   **The Problem:** Manual data entry and document processing are expensive due to labor costs and potential errors.\n",
      "*   **OCR Solution:** By automating these tasks, OCR can significantly reduce labor costs and improve accuracy, leading to cost savings.\n",
      "*   **Real-World Example:** A government agency implements OCR to automate the processing of tax forms, saving taxpayer money and improving efficiency.\n",
      "\n",
      "**In Summary:**\n",
      "\n",
      "OCR is useful because it bridges the gap between the physical and digital worlds.  It takes information locked in images and printed documents and transforms it into a usable, searchable, and editable format.  This leads to increased efficiency, reduced costs, improved accessibility, and better decision-making in a wide range of applications. The core benefit is **making information more accessible and actionable.**\n",
      "\n",
      "role - user: Tell me about OCR\n",
      "role - model: Okay, let's dive into the world of OCR: Optical Character Recognition.\n",
      "\n",
      "**What is OCR?**\n",
      "\n",
      "At its core, Optical Character Recognition (OCR) is a technology that allows computers to \"read\" text from images, scanned documents, or even handwriting.  It essentially converts images of text into machine-readable text data. Think of it as giving computers the ability to understand written language in a visual format.\n",
      "\n",
      "**How does it work?**\n",
      "\n",
      "The OCR process typically involves these key steps:\n",
      "\n",
      "1.  **Image Acquisition:** The process starts with an image containing text. This could be a scanned document, a photo taken with a camera, or even an image from a PDF file.\n",
      "\n",
      "2.  **Preprocessing:** This stage enhances the image to improve OCR accuracy. Common preprocessing steps include:\n",
      "    *   **Deskewing:** Correcting any slant or rotation in the image.\n",
      "    *   **Noise Reduction:** Removing unwanted specks, shadows, or other imperfections.\n",
      "    *   **Binarization:** Converting the image to black and white, making it easier to distinguish characters from the background.\n",
      "    *   **Line Removal/Correction:** Isolating the lines of text and attempting to straighten warped lines.\n",
      "\n",
      "3.  **Character Segmentation:** This involves identifying and isolating individual characters (letters, numbers, symbols) within the image. This can be a challenging step, especially with connected or poorly formed characters.\n",
      "\n",
      "4.  **Character Recognition:** This is where the magic happens. The OCR engine compares each isolated character to a database of known character shapes (fonts and styles). It uses various techniques, including:\n",
      "    *   **Pattern Matching:** Comparing the character to a pre-defined set of character templates.\n",
      "    *   **Feature Extraction:** Identifying key features of the character (e.g., lines, curves, loops) and using those features to classify the character.\n",
      "    *   **Machine Learning (Deep Learning):** Modern OCR systems increasingly rely on machine learning models, especially Convolutional Neural Networks (CNNs), which are trained on vast datasets of images and text to recognize characters with high accuracy. Deep learning models can also handle variations in fonts, styles, and image quality.\n",
      "\n",
      "5.  **Post-processing:** This stage cleans up the recognized text, correcting errors and improving readability. This might involve:\n",
      "    *   **Spell Checking:** Identifying and correcting misspelled words.\n",
      "    *   **Contextual Analysis:** Using the surrounding words to determine the most likely correct character, especially when the OCR engine is unsure.\n",
      "    *   **Formatting Restoration:** Attempting to reconstruct the original layout and formatting of the document (e.g., headings, paragraphs, tables).\n",
      "\n",
      "**Uses and Applications of OCR:**\n",
      "\n",
      "OCR has a wide range of applications across various industries:\n",
      "\n",
      "*   **Document Management:** Converting scanned documents into searchable and editable files, reducing paper clutter, and improving accessibility.\n",
      "*   **Data Entry Automation:** Extracting data from forms, invoices, receipts, and other documents, automating data entry processes, and reducing manual labor.\n",
      "*   **Banking and Finance:** Processing checks, loan applications, and other financial documents.\n",
      "*   **Healthcare:** Extracting information from patient records, medical reports, and insurance claims.\n",
      "*   **Legal:** Reviewing and analyzing legal documents, contracts, and evidence.\n",
      "*   **Library and Archival:** Digitizing books, newspapers, and other historical documents, preserving them for future generations.\n",
      "*   **Accessibility:** Making printed materials accessible to people with disabilities by converting them to text-to-speech or braille formats.\n",
      "*   **Mobile Apps:** Capturing text from signs, menus, and other real-world objects using smartphone cameras. Translation apps use OCR to read foreign text and translate it.\n",
      "*   **License Plate Recognition (LPR):** Used in parking systems, toll booths, and law enforcement to automatically identify vehicle license plates.\n",
      "*   **Robotics and Automation:** Enabling robots to \"read\" labels, instructions, and other information in their environment.\n",
      "*   **Invoice Processing:** Automating the extraction of key data (supplier name, invoice number, amounts, etc.) from invoices.\n",
      "*   **Automated Form Processing:** Extracting data from structured or semi-structured forms.\n",
      "\n",
      "**Advantages of OCR:**\n",
      "\n",
      "*   **Increased Efficiency:** Automates tasks that would otherwise require manual data entry, saving time and reducing errors.\n",
      "*   **Improved Accuracy:** Modern OCR systems can achieve very high accuracy rates, especially with clean, well-formatted documents.\n",
      "*   **Reduced Costs:** Reduces labor costs associated with manual data entry and document processing.\n",
      "*   **Enhanced Accessibility:** Makes printed materials accessible to people with disabilities.\n",
      "*   **Better Searchability:** Allows you to search and retrieve information from scanned documents.\n",
      "*   **Space Saving:** Reduces the need for physical storage of paper documents.\n",
      "\n",
      "**Limitations of OCR:**\n",
      "\n",
      "*   **Accuracy Issues:** OCR accuracy can be affected by poor image quality, complex layouts, unusual fonts, handwriting, and damage to the original document.\n",
      "*   **Font and Language Support:** Some OCR engines may not support all fonts and languages.\n",
      "*   **Cost:** Commercial OCR software can be expensive.\n",
      "*   **Complexity:** Setting up and configuring OCR systems can be complex, especially for large-scale applications.\n",
      "*   **Handwriting Recognition:** While OCR has improved, handwriting recognition is still challenging and less accurate than printed text recognition.  The variability of handwriting styles makes it difficult to achieve high accuracy.\n",
      "*   **Table Extraction:** Extracting data from tables can be difficult, especially with complex or irregular table structures.\n",
      "\n",
      "**Types of OCR Software and Engines:**\n",
      "\n",
      "*   **Commercial OCR Software:** ABBYY FineReader, Readiris, OmniPage.  These are often desktop applications with advanced features and high accuracy.\n",
      "*   **Online OCR Services:** Google Cloud Vision API, Microsoft Azure Computer Vision API, Amazon Textract. These are cloud-based services that offer OCR as an API, allowing developers to integrate OCR functionality into their applications.  They often operate on a pay-per-use basis.\n",
      "*   **Open-Source OCR Engines:** Tesseract OCR (one of the most popular open-source engines, backed by Google), OCRopus.\n",
      "*   **OCR Libraries and Toolkits:** Developers can use OCR libraries and toolkits to build custom OCR applications.\n",
      "\n",
      "**Factors Affecting OCR Accuracy:**\n",
      "\n",
      "*   **Image Quality:** High-resolution, clear images with good contrast are essential for accurate OCR.\n",
      "*   **Font Type and Size:** Simple, common fonts are easier to recognize than complex or unusual fonts.  Larger font sizes generally lead to better accuracy.\n",
      "*   **Document Layout:** Simple, well-structured layouts are easier to process than complex layouts with multiple columns, tables, and images.\n",
      "*   **Language:** The OCR engine must be trained to recognize the language of the text.\n",
      "*   **Noise and Distortion:** Noise, distortion, and other imperfections in the image can negatively affect OCR accuracy.\n",
      "*   **Skew:** Skewed images (images that are rotated) can cause errors in OCR.  Deskewing is an important preprocessing step.\n",
      "\n",
      "**Key Takeaways:**\n",
      "\n",
      "*   OCR is a powerful technology that converts images of text into machine-readable text.\n",
      "*   It has a wide range of applications in various industries.\n",
      "*   OCR accuracy depends on factors such as image quality, font, and document layout.\n",
      "*   There are various types of OCR software and engines available, from commercial desktop applications to cloud-based APIs and open-source options.\n",
      "*   Modern OCR is increasingly powered by Machine Learning/Deep Learning, significantly improving the accuracy of character recognition.\n",
      "\n",
      "If you have any more specific questions about OCR, feel free to ask! For example, you might want to know about specific OCR software, handwriting recognition, or OCR APIs.  Let me know what interests you!\n",
      "\n",
      "role - user: How is it useful\n",
      "role - model: Okay, let's break down the usefulness of OCR into concrete examples, highlighting both personal and professional applications:\n",
      "\n",
      "**1. Saving Time and Increasing Efficiency:**\n",
      "\n",
      "*   **The Problem:** Manually typing information from documents is incredibly time-consuming and prone to errors. Imagine needing to extract all the invoice numbers from a stack of 100 invoices.\n",
      "*   **OCR Solution:** OCR software can scan those invoices and automatically extract the invoice numbers into a spreadsheet in minutes. This saves countless hours of manual data entry and eliminates potential typos.\n",
      "*   **Real-World Example:** An accounting department processing hundreds of invoices daily uses OCR to automate data entry, freeing up staff to focus on more strategic tasks.\n",
      "\n",
      "**2. Enhanced Document Management and Searchability:**\n",
      "\n",
      "*   **The Problem:** Paper documents are difficult to search and organize. Finding a specific piece of information within a large archive of paper files can be a nightmare.\n",
      "*   **OCR Solution:** OCR converts scanned documents into searchable PDFs.  This means you can search for specific keywords or phrases within the document, even though it originated as an image.\n",
      "*   **Real-World Example:** A law firm digitizes all its legal documents using OCR, allowing lawyers to quickly find relevant case files and precedents by searching for keywords.\n",
      "\n",
      "**3. Automating Data Entry and Business Processes:**\n",
      "\n",
      "*   **The Problem:** Manual data entry is a bottleneck in many business processes, such as processing orders, handling customer inquiries, and managing inventory.\n",
      "*   **OCR Solution:** OCR can be integrated into business workflows to automate data extraction from forms, invoices, receipts, and other documents. This data can then be automatically fed into databases, CRM systems, and other business applications.\n",
      "*   **Real-World Example:** An e-commerce company uses OCR to automatically process order forms received via email, reducing order processing time and improving customer satisfaction.\n",
      "\n",
      "**4. Making Information Accessible to People with Disabilities:**\n",
      "\n",
      "*   **The Problem:** Printed materials are inaccessible to people with visual impairments or other disabilities.\n",
      "*   **OCR Solution:** OCR can convert printed text into digital text that can be read aloud by screen readers or displayed in braille, making information accessible to a wider audience.\n",
      "*   **Real-World Example:** A university uses OCR to convert textbooks and course materials into accessible formats for students with visual impairments.\n",
      "\n",
      "**5. Mobile Productivity and Convenience:**\n",
      "\n",
      "*   **The Problem:**  Copying text from printed materials or images on your phone can be tedious.\n",
      "*   **OCR Solution:**  Mobile OCR apps allow you to take a picture of text (e.g., a sign, a restaurant menu) and instantly convert it into editable text on your phone. This makes it easy to copy and paste text into emails, notes, or other applications.\n",
      "*   **Real-World Example:** A traveler uses a mobile OCR app to translate foreign language menus or signs while abroad.\n",
      "\n",
      "**6. Improved Customer Service:**\n",
      "\n",
      "*   **The Problem:**  Customers may submit forms, documents, or images containing important information. Manually processing these submissions can be slow and inefficient.\n",
      "*   **OCR Solution:**  OCR can automatically extract information from these submissions, allowing customer service representatives to quickly access and process customer requests.\n",
      "*   **Real-World Example:**  An insurance company uses OCR to process claims forms submitted by customers, speeding up the claims process and improving customer satisfaction.\n",
      "\n",
      "**7. Digitizing and Preserving Historical Documents:**\n",
      "\n",
      "*   **The Problem:**  Old and fragile documents are at risk of deterioration and loss.\n",
      "*   **OCR Solution:**  OCR allows libraries and archives to digitize historical documents, preserving them for future generations.\n",
      "*   **Real-World Example:**  A historical society digitizes its collection of old letters and manuscripts using OCR, making them accessible to researchers and the public online.\n",
      "\n",
      "**8. Cost Reduction:**\n",
      "\n",
      "*   **The Problem:** Manual data entry and document processing are expensive due to labor costs and potential errors.\n",
      "*   **OCR Solution:** By automating these tasks, OCR can significantly reduce labor costs and improve accuracy, leading to cost savings.\n",
      "*   **Real-World Example:** A government agency implements OCR to automate the processing of tax forms, saving taxpayer money and improving efficiency.\n",
      "\n",
      "**In Summary:**\n",
      "\n",
      "OCR is useful because it bridges the gap between the physical and digital worlds.  It takes information locked in images and printed documents and transforms it into a usable, searchable, and editable format.  This leads to increased efficiency, reduced costs, improved accessibility, and better decision-making in a wide range of applications. The core benefit is **making information more accessible and actionable.**\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chat = client.chats.create(model=\"gemini-2.0-flash\")\n",
    "\n",
    "response = chat.send_message(\"Tell me about OCR\")\n",
    "print(response.text)\n",
    "\n",
    "response = chat.send_message(\"How is it useful\")\n",
    "print(response.text)\n",
    "\n",
    "for message in chat.get_history():\n",
    "    print(f'role - {message.role}',end=\": \")\n",
    "    print(message.parts[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, let's analyze Figure 1 from the provided OCR text.\n",
      "\n",
      "**Figure 1 Analysis:**\n",
      "\n",
      "1.  **Title:** \"Dataset Splits for Each Language\"\n",
      "2.  **Type:** Grouped Bar Chart.\n",
      "3.  **X-axis:** \"Language\", showing the four languages used in the study: English, German, French, and Python.\n",
      "4.  **Y-axis:** \"Number of Tokens (million)\", indicating the volume of data measured in millions of tokens. The scale ranges from 0 to 140 million.\n",
      "5.  **Legend:** \"Dataset Splits\", identifying the three different colored bars for each language:\n",
      "    *   Blue: Training Set\n",
      "    *   Orange: Validation Set\n",
      "    *   Green: Test Set\n",
      "6.  **Content:** The chart visually represents the size (in millions of tokens) of the training, validation, and test datasets for each of the four languages.\n",
      "7.  **Observations:**\n",
      "    *   **Training Data Dominance:** For all languages, the training set (blue bar) is significantly larger than the validation (orange) and test (green) sets, which is standard practice in machine learning.\n",
      "    *   **Language Sizes (Training):**\n",
      "        *   English has the largest training set, approaching 140 million tokens.\n",
      "        *   German is the second largest, around 120 million tokens.\n",
      "        *   French and Python have roughly similar training set sizes, both slightly over 100 million tokens.\n",
      "    *   **Validation/Test Data Sizes:** These sets are considerably smaller, generally appearing below 20 million tokens for each language. The relative proportions seem consistent across languages.\n",
      "    *   **Dataset Balance:** While the text (Section 3.1) mentions efforts to ensure balanced *token counts* across languages in the *source* datasets, this chart shows the actual token counts used. The training sets are not perfectly identical in size but are within a comparable range (approx. 100M-140M tokens), suggesting a relatively balanced dataset in terms of token volume across the languages, especially considering potential vast differences in raw source data availability.\n",
      "8.  **Contextual Relevance:** This figure directly supports Section 3.1 (Dataset Preparation) and 3.2 (Tokenization) by providing a visual breakdown of the multilingual dataset used for training and evaluating the models (teacher, students, router) discussed in the paper. It quantifies the data foundation upon which the experiments are built.\n",
      "\n",
      "**In summary:** Figure 1 effectively visualizes the composition and scale of the multilingual dataset used in the research, breaking it down by language and dataset split (training, validation, test) based on the number of tokens. It highlights the substantial size of the training data compared to evaluation sets and shows the relative distribution of data across the four languages.\n"
     ]
    }
   ],
   "source": [
    "# Retrieve and encode the PDF byte\n",
    "filepath = pdf1_filepath\n",
    "\n",
    "prompt = \"Analyze Figure 1\"\n",
    "response = client.models.generate_content(\n",
    "  model=models[1],\n",
    "  contents=[\n",
    "      types.Part.from_bytes(\n",
    "        data=filepath.read_bytes(),\n",
    "        mime_type='application/pdf',\n",
    "      ),\n",
    "      prompt])\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time to Convert the PDF to Markdown\n",
    "- Write Prompt\n",
    "- Call Gemini with Prompt\n",
    "- Refine Prompts and Repeat Until Satisfactory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "promptv1 = \"\"\"Convert the following PDF to Markdown.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```markdown\n",
      "arXiv:2407.19610v1 [cs.AI] 28 Jul 2024\n",
      "\n",
      "# MIXTURE OF MODULAR EXPERTS: DISTILLING KNOWLEDGE FROM A MULTILINGUAL TEACHER INTO SPECIALIZED MODULAR LANGUAGE MODELS\n",
      "\n",
      "**Mohammed Al-Maamari**\n",
      "Chair of Data Science\n",
      "University of Passau\n",
      "Germany, Passau\n",
      "Mohammed.Al-Maamari@uni-passau.de\n",
      "\n",
      "**Mehdi Ben Amor**\n",
      "Chair of Data Science\n",
      "University of Passau\n",
      "Germany, Passau\n",
      "Mehdi.BenAmor@uni-passau.de\n",
      "\n",
      "**Michael Granitzer**\n",
      "Chair of Data Science\n",
      "University of Passau\n",
      "Germany, Passau\n",
      "Michael.Granitzer@uni-passau.de\n",
      "\n",
      "July 30, 2024\n",
      "\n",
      "## ABSTRACT\n",
      "\n",
      "This research explores the integration of Knowledge Distillation (KD) and Mixture of Experts (MoE) to create modular, efficient, and specialized multilingual language models. The primary objectives include evaluating adaptive versus fixed alpha methods in KD, developing and comparing modular MoE architectures in handling multi-domain inputs and preventing catastrophic forgetting.\n",
      "\n",
      "We address the computational challenges of large language models (LLMs) and the need for modular models. KD compresses LLMs into smaller models for efficiency, while MoE architectures enhance modularity by combining multiple experts for specialized tasks. Experiments showed that adaptive and fixed alpha methods in KD yielded similar performance, with marginal improvements from adaptive alpha. The combined loss approach slightly outperformed alternating losses, providing more stable learning. The router, trained to classify input sequences into English, French, German, or Python, achieved high accuracy precision, recall, and F1 score of 99.95%, with Logistic Regression as the most effective classifier.\n",
      "\n",
      "Evaluation of modular MoE architectures revealed that the Pre-trained Language Experts (PLE) setup and Joint Expert Embedding Training (JEET) demonstrated similar performance, while the MoE with Common Expert (MoE-CE) setup showed slightly lower performance. However, when including a common expert in MoE-CE, its performance approaches that of both PLE and JEET. The study on catastrophic forgetting indicated that sequential training led to significant forgetting, while single-session training with balanced batches approach, and MoE approach mitigated this issue effectively. The MoE architecture preserved knowledge across multiple languages, demonstrating its effectiveness. We open-sourced the dataset [^1], the balanced dataset creation tool [^2], and the research codebase [^3].\n",
      "\n",
      "## 1 Introduction\n",
      "\n",
      "Language models (LMs) are pivotal in Natural Language Processing (NLP), facilitating a variety of tasks such as machine translation [1], sentiment analysis [2], and text generation [3]. Despite their potential, large-scale models encounter challenges like computational inefficiency, limited adaptability, and catastrophic forgetting. Our study explores the amalgamation of Knowledge Distillation (KD) and Mixture of Experts (MoE) to mitigate these challenges, aiming to improve efficiency, modularity, and specialization in language models.\n",
      "\n",
      "Transformers, the backbone of many large models, require substantial computational resources [4], which hampers their scalability and accessibility. The increasing complexity and size associated with supporting more languages and domains adversely affect training durations and generalization abilities [5]. Additionally, fine-tuning for specific tasks consumes significant resources and often falls short of achieving optimal outcomes [6]. Catastrophic forgetting is a major hurdle, particularly in models handling multiple languages and domains, as they tend to lose previously acquired knowledge when exposed to new data [7].\n",
      "\n",
      "Specialized models, when trained on narrow domains such as programming languages, have demonstrated superiority in specific tasks like code completion and bug detection over their general-purpose counterparts [8]. Introducing modularity into neural network design enhances flexibility, scalability, and maintainability, enabling updates to individual network segments without necessitating a complete retraining.\n",
      "\n",
      "This research primarily focuses on exploring various integration strategies of KD and MoE to create specialized, efficient, and modular language models. While we employed straightforward knowledge distillation techniques, reaching state-of-the-art knowledge distillation was not our objective. Instead, our primary goal was to investigate the feasibility of different integration methods of KD and MoE. KD is the process where smaller student models learn to mimic the behavior of a larger, more capable teacher model using their probabilistic outputs [9]. MoE architectures, on the other hand, dynamically delegate tasks to specialized models, thereby enhancing performance across varied domains and languages [10].\n",
      "\n",
      "Our research objectives include evaluating adaptive versus fixed alpha methods in KD, training a router to efficiently direct inputs to the appropriate experts, and comparing various MoE architectures to determine their effectiveness in handling multi-domain inputs and in averting catastrophic forgetting. This study contributes to the development of more adept and effective NLP systems that can support a broad spectrum of applications.\n",
      "\n",
      "## 2 Related Work\n",
      "\n",
      "### 2.1 Knowledge Distillation\n",
      "\n",
      "\"*DistilBERT*\" by Sanh et al. [11] introduces a method to distill BERT into a smaller, faster model while retaining most of its performance. By leveraging a triple loss function (language modeling, distillation, and cosine-distance losses), DistilBERT reduces model size by 40% and maintains 97% of BERT's language understanding capabilities. This approach makes the model suitable for deployment in environments with constrained computational resources. DistilBERT's training involves using every second layer from the teacher model to retain inductive biases and employs techniques like gradient accumulation and dynamic masking.\n",
      "\n",
      "\"*MiniLLM*\" by Gu et al. [12] proposes using reverse Kullback-Leibler divergence (KLD) for knowledge distillation to address the issue of overestimating low-probability regions in the teacher model's distribution. The method minimizes the difference between the teacher and student model distributions using a conditional text generation framework. Techniques such as single-step decomposition and teacher-mixed sampling are employed to stabilize and accelerate the training process. Our work adopts a simplified version of reverse KLD at the word level, focusing on efficiently capturing the teacher model's probabilistic characteristics.\n",
      "\n",
      "### 2.2 Mixture of Experts\n",
      "\n",
      "\"*Mixtral of Experts*\" by Jiang et al. [13] presents Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model. The model dynamically selects two out of eight feedforward blocks per token at each layer, optimizing computational resource usage. Mixtral's transformer model incorporates MoE layers with a routing mechanism to allocate tokens to experts. Evaluations show superior performance in various benchmarks, highlighting the model's efficiency and effectiveness.\n",
      "\n",
      "\"*Branch-Train-MiX*\" by Sukhbaatar et al. [14] investigates methods for training LLMs across multiple specialized domains. The Branch-Train-MiX (BTX) method involves branching from a seed model, training domain-specific experts, and integrating them into a unified MoE model. This approach improves training efficiency and model performance by leveraging parallelism and specialization. BTX outperforms baselines like Llama-2 in accuracy and computational efficiency.\n",
      "\n",
      "\"*Branch-Train-Merge*\" by Li et al. [15] introduces the Branch-Train-Merge (BTM) algorithm, which enhances the efficiency of training large language models. BTM facilitates independent training of subparts of the model on different data subsets, reducing communication overhead. The approach involves three steps: branching, training, and merging. BTM achieves improved perplexities and higher updates per second due to reduced communication overhead, making it a scalable and efficient training paradigm.\n",
      "\n",
      "Our research integrates MoE with Knowledge Distillation (KD) to develop specialized multilingual models. Unlike Mixtral and BTX, which focus on token-level routing and parallel training of domain-specific experts, our work emphasizes sequence-level routing and the integration of KD with MoE. This approach aims to address multi-domain adaptability and reduce catastrophic forgetting, contributing to the development of modular and efficient language models.\n",
      "\n",
      "## 3 Methods\n",
      "\n",
      "This section describes the methodologies, tools, and algorithms used in this research, focusing on dataset preparation, model training, knowledge distillation techniques, and MoE architecture design.\n",
      "\n",
      "### 3.1 Dataset Preparation\n",
      "\n",
      "The dataset comprises multilingual text data in English, German, French, and Python code. The primary sources are the Wiki40B dataset [16] for natural languages and the CodeParrot GitHub \"codeparrot/github-code-clean\" dataset [17] for Python code. The Wiki40B dataset includes 2,926,536 English, 1,227,206 French, and 1,554,908 German training samples, ensuring balanced token counts across these languages. Python data contains 645K Python code files, filtered and balanced based on code length to ensure comprehensive coverage.\n",
      "\n",
      "**Table 1: Number of Articles per Language**\n",
      "| Language | Training Samples | Validation Samples |\n",
      "|---|---|---|\n",
      "| English | 2,926,536 | 163,597 |\n",
      "| French | 1,227,206 | 68,655 |\n",
      "| German | 1,554,908 | 86,068 |\n",
      "\n",
      "### 3.2 Tokenization\n",
      "\n",
      "Byte Pair Encoding (BPE) [18] was used for tokenization, with a vocabulary size of 32,000 tokens. The tokenizer was trained on a balanced dataset of the four languages, using special tokens such as `<unk>`, `<s>`, and `</s>`. This setup ensured efficient vocabulary usage and compatibility with the models.\n",
      "\n",
      "*[Image: Bar chart showing the number of tokens (in millions) for Training, Validation, and Test sets for English, German, French, and Python. English: ~135M Training, ~18M Validation, ~18M Test. German: ~120M Training, ~18M Validation, ~18M Test. French: ~105M Training, ~15M Validation, ~15M Test. Python: ~102M Training, ~14M Validation, ~14M Test.]*\n",
      "**Figure 1: Dataset Splits for Each Language**\n",
      "\n",
      "### 3.3 Teacher Model Training\n",
      "\n",
      "The teacher model, a GPT-2 Medium with 340 million parameters, was trained on the multilingual dataset. Initial attempts with Mistral 1.6B and Phi 1.34B configurations faced stability issues, leading to the selection of GPT-2 Medium for its balance of performance and computational efficiency. The training process involved a context length of 1024 tokens, a virtual batch size of 512, and optimization techniques such as gradient accumulation and clipping. Training was conducted on 2 A100 GPUs using DeepSpeed and Accelerate libraries.\n",
      "\n",
      "### 3.4 Knowledge Distillation\n",
      "\n",
      "Knowledge distillation (KD) involved transferring knowledge from the GPT-2 Medium teacher model to smaller student models. The student models were trained to replicate the teacher's outputs using a combined loss function incorporating Cross-Entropy Loss and Reverse Kullback-Leibler (RKL) Divergence Loss [12]. The adaptive alpha method dynamically adjusted the weights of these losses based on training progress.\n",
      "\n",
      "*[Image: Line graph showing training loss vs. steps for three models: Mistral 1.6B (high initial loss, noisy, decreases but remains high ~7), Phi 1.34B (starts lower, noisy, around 6-7), Mistral GPT-M 440M (starts lowest, smoother decrease, settles around 3).]*\n",
      "**Figure 2: Learning Curve for Mistral 1.6B, Mistral GPT-M 440M, Phi 1.34B**\n",
      "\n",
      "$L_{total} = \\alpha \\cdot L_{LM} + \\beta \\cdot L_{KD}$ (1)\n",
      "\n",
      "### 3.5 Mixture of Experts Architecture\n",
      "\n",
      "The Mixture of Experts (MoE) architecture includes multiple specialized sub-models (experts) and a router mechanism. Three setups were evaluated:\n",
      "\n",
      "1.  **Setup 1: Pre-trained Language Experts (PLE)** - Experts pre-trained independently via KD from the teacher model, handling specific languages.\n",
      "2.  **Setup 2: Joint Expert Embedding Training (JEET)** - Experts trained concurrently with a shared embedding layer, maintaining modularity.\n",
      "3.  **Setup 3: MoE with Common Expert (MoE-CE)** - Includes a common expert trained on all languages, sharing the embedding layer with specialized experts.\n",
      "\n",
      "### 3.6 Router\n",
      "\n",
      "The router, pre-trained to classify inputs into English, French, German, or Python, achieved high accuracy (99.95%) using TF-IDF vectorization and Logistic Regression. It dynamically selects the appropriate expert during inference, optimizing the allocation of computational resources.\n",
      "\n",
      "*[Image: Diagram showing the Knowledge Distillation process. Dataset feeds into Teacher model, producing Teacher Logits. Teacher Logits and Student Logits (from Student model fed by Dataset) calculate KD Loss. Student Logits also calculate LM Loss. An adaptive alpha calculation combines KD Loss and LM Loss into a Combined Loss used to update the Student.]*\n",
      "**Figure 3: Our Knowledge Distillation Process**\n",
      "\n",
      "*[Image: Diagram showing the JEET MoE architecture. Input goes to a Router. The Router directs the input representation (via Shared Embeddings) to one of several Experts (Expert 1, Expert 2, Expert 3, ..., Expert n). The output from the selected expert is the final Output.]*\n",
      "**Figure 4: Architecture of the Joint Expert Embedding Training MoE Setup**\n",
      "\n",
      "### 3.7 Training and Inference\n",
      "\n",
      "During training, data was batched by language, and the router directed each batch to the corresponding expert. This approach ensured specialization without interference. During inference, the system handled mixed-language batches, maintaining efficiency and specialization.\n",
      "\n",
      "**Table 2: Model Configurations and Hyperparameters**\n",
      "| Model         | Parameters | Configuration Source | Hyperparameters |\n",
      "|---------------|------------|----------------------|-----------------|\n",
      "| Mistral 1.6B  | 1.6B       | Phi1.5               | Custom          |\n",
      "| Phi 1.34B     | 1.34B      | Phi1.5               | Standard        |\n",
      "| GPT-2 110M    | 110M       | GPT-2                | Standard        |\n",
      "| GPT-2 340M    | 340M       | GPT-2                | Standard        |\n",
      "| Distil-GPT2   | 82M        | Distil-GPT2          | Standard        |\n",
      "\n",
      "*[Image: Diagram showing the MoE with Common Expert architecture. Similar to Figure 4, but includes a 'Common Expert' alongside the specialized experts (Expert 1-n). The Router can potentially select the Common Expert or one of the specialized experts via the Shared Embeddings.]*\n",
      "**Figure 5: Architecture of the MoE with Common Expert Setup**\n",
      "\n",
      "*[Image: Diagram showing the Pre-trained Language Experts architecture. Input goes to a Router. The Router selects one of several independent, pre-trained Experts (Expert 1, Expert 2, Expert 3, ..., Expert n), each presumably with its own embeddings. The selected Expert produces the Output.]*\n",
      "**Figure 6: Architecture of the Pre-trained Language Experts**\n",
      "\n",
      "## 4 Results\n",
      "\n",
      "The experiments focused on evaluating different aspects of the Mixture of Experts (MoE) architecture, including Knowledge Distillation (KD) methods, router training, and the impact of modular MoE architectures on system performance.\n",
      "\n",
      "### 4.1 Experimental Setup\n",
      "\n",
      "The experimental setup involved two distinct KD approaches: independent KD and sequential KD. Independent KD trained separate student models for each language (English, French, German, and Python) using the outputs of the teacher model. Sequential KD distilled a single student model sequentially with knowledge from each language. The evaluation metrics used were perplexity and cross-entropy loss. The training hyperparameters, including a context length of 1024 tokens and a vocabulary size of 32,000, were consistent with those used for the teacher model. The MoE training utilized the same tokenizer and frameworks such as DeepSpeed and Accelerate to manage computational demands.\n",
      "\n",
      "### 4.2 Adaptive vs. Fixed Alpha\n",
      "\n",
      "**Research Question 1 (RQ1):** What is the effectiveness of adaptive vs. fixed alpha methods in Knowledge Distillation?\n",
      "\n",
      "The adaptive alpha method, which dynamically adjusts weights during training, was hypothesized to perform better than the fixed alpha method. The results, shown in Figure 7, indicated that the adaptive alpha method outperformed the fixed alpha method by a small margin (0.01 improvement in evaluation loss). Testing different fixed alpha values showed that an alpha of 0.5 performed nearly as well as the adaptive method, suggesting minimal benefits of dynamic adjustment in this setup.\n",
      "\n",
      "*[Image: Line graph showing Evaluation Loss vs. Step for different alpha settings. Alpha=0.7, Alpha=0.5, Alpha=0, and Adaptive alpha are plotted. All lines decrease over 600 steps. Adaptive alpha and Alpha=0.5 show the lowest final loss, very close to each other (around 5.2). Alpha=0.7 is slightly higher, and Alpha=0 is significantly higher (~7.1). A zoomed inset highlights the small difference between Adaptive alpha and Alpha=0.5 at step 600.]*\n",
      "**Figure 7: Evaluation loss for Adaptive Alpha and different values of Fixed Alpha**\n",
      "\n",
      "### 4.3 Alternating Losses (AL) VS Combined Losses (CL)\n",
      "\n",
      "**Research Question 2 (RQ2):** What is the impact of alternating losses during training on model convergence and performance?\n",
      "\n",
      "The hypothesis was that alternating `KD_loss` and `LM_loss` would enhance model performance. However, the results, shown in Figure 8, indicated that the combined loss method performed slightly better, with an evaluation loss marginally lower than the alternating losses method (4.305 vs. 4.322). This suggests that consistently applying both losses at each training step provides a more effective learning signal.\n",
      "\n",
      "*[Image: Line graph showing Evaluation Loss vs. Step for Combined Loss (CL Eval En Loss) and Alternating Losses (AL Eval En Loss). Both lines decrease from ~6.1 down to ~4.3 over 600 steps. The CL line is consistently slightly below the AL line.]*\n",
      "**Figure 8: Evaluation Loss for Combined Loss vs. Alternating Losses Methods**\n",
      "\n",
      "### 4.4 The Router\n",
      "\n",
      "**Research Question 3 (RQ3):** Can we train a router that accurately routes input sequences to one of the four experts, classifying the input sequence into one of the four classes [En, Fr, De, Py]?\n",
      "\n",
      "Various classifiers were evaluated, with the Logistic Regression classifier achieving the highest performance (99.95% accuracy). The confusion matrix in Figure 9 and Table 3 confirmed the router's effectiveness in accurately classifying input sequences.\n",
      "\n",
      "**Table 3: Performance Metrics of Different Classifiers for Router Training**\n",
      "| Classifier          | Accuracy ↑ | Precision ↑ | Recall ↑ | F1 Score ↑ |\n",
      "|---------------------|------------|-------------|----------|------------|\n",
      "| Logistic Regression | 0.9995     | 0.9995      | 0.9995   | 0.9995     |\n",
      "| SGD Classifier      | 0.9993     | 0.9993      | 0.9993   | 0.9993     |\n",
      "| Random Forest       | 0.9995     | 0.9995      | 0.9995   | 0.9995     |\n",
      "\n",
      "### 4.5 Modular MoE Model Design\n",
      "\n",
      "**Research Question 4 (RQ4):** How do the three MoE architectures compare in terms of performance and robustness?\n",
      "\n",
      "Three MoE setups were compared: Pre-trained Language Experts (PLE), Joint Expert Embedding Training (JEET), and MoE with Common Expert (MoE-CE). The results, summarized in Table 4, showed that PLE achieved the best perplexities for English and German, while JEET performed best for French and Python. MoE-CE, when using the common expert, showed performance approaching that of PLE and JEET, highlighting the benefits of including a common expert.\n",
      "\n",
      "**Table 4: Perplexity Metrics for Different MoE Architectures**\n",
      "| Architecture    | English ↓ | French ↓ | German ↓ | Python ↓ |\n",
      "|-----------------|-----------|----------|----------|----------|\n",
      "| PLE             | **74.09** | 20.30    | **39.86**| 28.92    |\n",
      "| JEET            | 75.79     | **20.12**| 40.38    | **27.02**|\n",
      "| MoE-CE w/o CE   | 90.83     | 23.24    | 47.75    | 29.89    |\n",
      "| MoE-CE + CE     | 78.96     | 20.91    | 41.92    | 27.16    |\n",
      "\n",
      "*(Bold indicates best performance for that language based on text description)*\n",
      "\n",
      "### 4.6 Impact of Adding a Common Expert to the MoE System\n",
      "\n",
      "**Research Question 5 (RQ5):** Does adding a common expert improve the overall performance of the MoE system and the performance of each expert independently?\n",
      "\n",
      "*[Image: Confusion Matrix heatmap for the router. True labels (En, Fr, De, Py) on y-axis, Predicted labels (En, Fr, De, Py) on x-axis. The diagonal shows very high values (0.9995, 0.9994, 0.9996, 0.9998), indicating high accuracy. Off-diagonal values are very small (<= 0.0004).]*\n",
      "**Figure 9: Confusion Matrix for the Router**\n",
      "\n",
      "The addition of a common expert in MoE-CE generally improved performance, as shown in Table 5. The overall perplexity was reduced by 5.69 points with the inclusion of the common expert, confirming its positive impact.\n",
      "\n",
      "### 4.7 Catastrophic Forgetting in Modular MoE Architecture VS Non-Modular Approaches\n",
      "\n",
      "**Research Question 6 (RQ6):** How does the modular MoE architecture compare to non-modular approaches in terms of catastrophic forgetting?\n",
      "\n",
      "Three experiments were conducted to compare the impact of catastrophic forgetting:\n",
      "1.  Sequentially distilling knowledge into a single student model.\n",
      "2.  Distilling knowledge into a single student model in one session.\n",
      "3.  Employing the MoE architecture with four separate student models.\n",
      "\n",
      "The results, shown in Table 6, indicated significant catastrophic forgetting in the sequential setup (up to 38% in German), whereas both single session distillation and MoE showed no catastrophic forgetting. The evaluation loss comparison for sequential training and MoE architecture is shown in Figure 10.\n",
      "\n",
      "The results confirmed that the modular MoE architecture effectively mitigates catastrophic forgetting, supporting the hypothesis that modularity allows for the retention of knowledge across multiple languages.\n",
      "\n",
      "## 5 Discussion\n",
      "\n",
      "This section interprets and contextualizes the findings from our experiments, providing insights into the efficacy of different Knowledge Distillation (KD) methods and Mixture of Experts (MoE) architectures. It addresses the research questions, compares our results with existing literature, and reflects on the challenges and limitations encountered during the research.\n",
      "\n",
      "**Table 5: Perplexity scores for Different Inference Settings in MoE-CE**\n",
      "| CommonExpert | RoutableExperts | En ↓   | Fr ↓   | De ↓   | Py ↓   | All ↓  |\n",
      "|--------------|-----------------|--------|--------|--------|--------|--------|\n",
      "| ✔            | En, Fr, De, Py  | 78.96  | 20.91  | 41.92  | 27.16  | 42.24  |\n",
      "| ✘            | En, Fr, De, Py  | 90.83  | 23.24  | 47.75  | 29.89  | 47.93  |\n",
      "| ✔            | En, Py          | 78.96  | 36.55  | 79.95  | 27.16  | 55.65  |\n",
      "| ✘            | En, Py          | 90.83  | 116.04 | 325.28 | 29.89  | 140.51 |\n",
      "| ✔            | Fr, Py          | 139.09 | 20.91  | 77.64  | 27.16  | 66.20  |\n",
      "| ✘            | Fr, Py          | 534.64 | 23.24  | 305.62 | 29.89  | 223.35 |\n",
      "| ✔            | De, Py          | 140.33 | 37.10  | 41.92  | 27.16  | 61.63  |\n",
      "| ✘            | De, Py          | 532.25 | 124.18 | 47.75  | 29.89  | 183.52 |\n",
      "| ✔            | En              | 78.96  | 36.55  | 79.95  | 53.65  | 62.28  |\n",
      "| ✘            | En              | 90.83  | 116.04 | 325.28 | 203.50 | 183.91 |\n",
      "| ✔            | Fr              | 139.09 | 20.91  | 77.64  | 68.62  | 76.56  |\n",
      "| ✘            | Fr              | 534.64 | 23.24  | 305.62 | 577.22 | 360.18 |\n",
      "| ✔            | De              | 140.33 | 37.10  | 41.92  | 55.11  | 68.62  |\n",
      "| ✘            | De              | 532.25 | 124.18 | 47.75  | 240.68 | 236.22 |\n",
      "| ✔            | Py              | 113.47 | 38.18  | 80.44  | 27.16  | 64.81  |\n",
      "| ✘            | Py              | 284.49 | 141.93 | 379.81 | 29.89  | 209.03 |\n",
      "| ✔            | None            | 104.01 | 27.66  | 57.72  | 38.03  | 56.86  |\n",
      "\n",
      "**Table 6: Forgotten Knowledge in Different Experiments**\n",
      "| Experiment         | Language | Forgotten Knowledge ↓ |\n",
      "|--------------------|----------|-----------------------|\n",
      "|                    | English  | 0.499 (12.0%)         |\n",
      "| A (Sequential)     | French   | 0.851 (31.0%)         |\n",
      "|                    | German   | 1.301 (38.0%)         |\n",
      "|                    | Python   | N/A                   |\n",
      "|                    | English  | 0 (0%)                |\n",
      "| B (Single Session) | French   | 0 (0%)                |\n",
      "|                    | German   | 0 (0%)                |\n",
      "|                    | Python   | 0 (0%)                |\n",
      "|                    | English  | 0 (0%)                |\n",
      "| C (MoE)            | French   | 0 (0%)                |\n",
      "|                    | German   | 0 (0%)                |\n",
      "|                    | Python   | 0 (0%)                |\n",
      "\n",
      "### 5.1 Interpretation of Findings\n",
      "\n",
      "#### 5.1.1 Adaptive vs. Fixed Alpha\n",
      "\n",
      "The comparison between adaptive alpha and fixed alpha methods, discussed in subsection 4.2, revealed that both approaches yielded similar performance levels. The adaptive alpha approach showed a marginally better performance, but this advantage was minimal due to the consistency of the dataset used for training both the teacher and student models. The similarity in KD loss and cross-entropy loss changes likely contributed to the comparable performance of both methods.\n",
      "\n",
      "*[Image: Line graph comparing Evaluation Loss vs. Step for Sequential Training (KD lines) and MoE architecture (MoE lines) for EN, DE, FR, PY. KD lines (dashed) show loss increasing for previously learned languages when a new one starts training (e.g., KD of EN loss jumps up when FR training starts). MoE lines (solid with markers) show stable, low loss for each language throughout the process.]*\n",
      "**Figure 10: Evaluation Loss Comparison for Sequential Training (Experiment A) and MoE Architecture (Experiment C)**\n",
      "\n",
      "#### 5.1.2 Alternating Losses vs. Combined Losses\n",
      "\n",
      "The experiment comparing alternating losses (AL) and combined losses (CL), as described in subsection 4.3, indicated that the combined loss approach slightly outperformed the alternating loss approach. The minimal difference suggests that the simplicity of the combination method—weighted averaging of the losses—may have masked potential benefits of alternating losses. More sophisticated methods of loss alternation may yield more pronounced differences and warrant further exploration.\n",
      "\n",
      "#### 5.1.3 Router Performance\n",
      "\n",
      "The router's performance, detailed in subsection 4.4, achieved high accuracy, precision, recall, and F1 scores. The distinctiveness of the four classes (English, German, French, and Python) and the balanced dataset used for training contributed to this success. The results confirm the router's capability to accurately classify input sequences and support the MoE architecture's performance.\n",
      "\n",
      "#### 5.1.4 Modular MoE Model Design\n",
      "\n",
      "The performance comparison of the three MoE architectures, discussed in subsection 4.5, highlighted the strengths and limitations of each setup. Pre-trained Language Experts (PLE) and Joint Expert Embedding Training (JEET) performed comparably, with PLE excelling in English and German, and JEET in French and Python. MoE with Common Expert (MoE-CE) improved significantly with the inclusion of a common expert during inference, suggesting that a shared knowledge base can enhance performance in multi-language tasks.\n",
      "\n",
      "#### 5.1.5 Catastrophic Forgetting in Modular vs. Non-Modular Approaches\n",
      "\n",
      "The study on catastrophic forgetting, as outlined in subsection 4.7, demonstrated that sequential training led to significant forgetting of previously learned languages, while single-session training and MoE approaches effectively mitigated this issue. The MoE approach, which assigns dedicated experts to each language, completely eliminated catastrophic forgetting, highlighting the effectiveness of modular architectures in preserving knowledge across multiple domains.\n",
      "\n",
      "### 5.2 Comparison with Existing Literature\n",
      "\n",
      "Our use of reverse Kullback-Leibler divergence (RKL) for knowledge distillation aligns with the approach presented in \"*MiniLLM: Knowledge Distillation of Large Language Models*\" [12], which demonstrated improved alignment between student and teacher models. Our findings validate the efficacy of RKL in our multilingual and modular model context.\n",
      "\n",
      "In comparison to \"*Mixtral of Experts*\" [13], which uses a sparse MoE architecture, our approach emphasizes modularity and specialization. Our method allows for the utilization of any subset of experts without compromising their individual performance and includes a common expert to enhance performance across languages.\n",
      "\n",
      "Our research also contrasts with \"*Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM*\" [14], which focuses on parallel training of domain-specific experts. Our approach combines KD and MoE, resulting in smaller, efficient student experts that maintain high performance. This dual focus on modularity and efficiency distinguishes our research.\n",
      "\n",
      "### 5.3 Challenges and Limitations\n",
      "\n",
      "#### 5.3.1 Challenges\n",
      "\n",
      "One significant challenge was the computational resource constraints, which necessitated the use of techniques such as gradient accumulation and efficient memory management. Ensuring balanced representation of the dataset also posed a challenge, requiring meticulous preprocessing and cleaning.\n",
      "\n",
      "#### 5.3.2 Limitations\n",
      "\n",
      "The primary limitation is the scale of the dataset. Our training dataset size of 490 million tokens is considerably smaller than those used in training state-of-the-art language models. This limitation affects the generalizability of our findings to larger-scale applications. Additionally, the focus on a limited number of languages and a single programming language (Python) restricts the applicability of our findings.\n",
      "\n",
      "#### 5.3.3 Future Work\n",
      "\n",
      "Future work should focus on addressing these limitations and exploring the scalability of our approach to larger datasets and more diverse languages and domains. Further research on the adaptive alpha method could help refine its implementation and identify scenarios where its benefits are more pronounced. Optimizing the training and integration process for the MoE architecture and investigating its applicability to other languages and domains will also be crucial.\n",
      "\n",
      "## 6 Conclusion\n",
      "\n",
      "### 6.1 Summary of Contributions\n",
      "\n",
      "This research integrates Knowledge Distillation (KD) and Mixture of Experts (MoE) to develop modular, efficient, and specialized multilingual language models. The primary objectives were to evaluate adaptive versus fixed alpha methods in KD, compare modular MoE architectures, and address catastrophic forgetting.\n",
      "\n",
      "#### 6.1.1 Knowledge Distillation\n",
      "\n",
      "The experiments comparing adaptive and fixed alpha methods in KD revealed similar performance, with the adaptive alpha method providing a slight improvement. The combined loss approach offered more stable learning dynamics compared to alternating losses.\n",
      "\n",
      "#### 6.1.2 Mixture of Experts\n",
      "\n",
      "Three MoE architectures were assessed: Pre-trained Language Experts (PLE), Joint Expert Embedding Training (JEET), and MoE with Common Expert (MoE-CE). PLE and JEET performed similarly, while MoE-CE, without utilizing the common expert, lagged behind but demonstrated enhanced results with the inclusion of a common expert. This indicates the effectiveness of shared knowledge in improving performance across multiple languages.\n",
      "\n",
      "#### 6.1.3 Router Performance\n",
      "\n",
      "The router, employing Logistic Regression for classification, achieved high accuracy and reliability in selecting the appropriate expert model for inputs, with accuracy, recall, precision, and F1-score all at 99.95%.\n",
      "\n",
      "#### 6.1.4 Catastrophic Forgetting\n",
      "\n",
      "Sequential training resulted in significant catastrophic forgetting, whereas single-session training and the MoE approach effectively mitigated this issue. The modular MoE architecture preserved knowledge across multiple languages, preventing catastrophic forgetting.\n",
      "\n",
      "### 6.2 Implications and Impact\n",
      "\n",
      "The integration of KD with MoE facilitates the development of modular, specialized, and efficient models that perform well across diverse tasks. The modularity of the MoE architecture enhances flexibility, allowing for the addition of new experts without retraining the entire system and addressing catastrophic forgetting by enabling the model to retain knowledge across multiple languages and domains.\n",
      "\n",
      "### 6.3 Challenges and Limitations\n",
      "\n",
      "The research was constrained by computational resources and a relatively small dataset of 490 million tokens, limiting the generalizability of the findings. Additionally, the focus on a limited number of languages and a single programming language indicates that further experimentation is needed to extend the approach to other languages, domains, and modalities.\n",
      "\n",
      "### 6.4 Future Work\n",
      "\n",
      "Future research should aim to scale the approach to larger datasets and more diverse languages and domains. Optimizing the training and integration process for the MoE architecture and exploring the applicability of the methods to other contexts are recommended. Further investigation into adaptive alpha methods and advanced loss functions for the common expert could provide deeper insights and enhance model performance.\n",
      "\n",
      "## References\n",
      "\n",
      "[1] Gennadi Lembersky, Noam Ordan, and Shuly Wintner. Language models for machine translation: Original vs. translated texts. *Computational Linguistics*, 38(4):799–825, 2012.\n",
      "\n",
      "[2] Wenxuan Zhang, Yue Deng, Bing Liu, Sinno Jialin Pan, and Lidong Bing. Sentiment analysis in the era of large language models: A reality check. *arXiv preprint arXiv:2305.15005*, 2023.\n",
      "\n",
      "[3] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. *Advances in neural information processing systems*, 33:1877–1901, 2020.\n",
      "\n",
      "[4] Prakhar Ganesh, Yao Chen, Xin Lou, Mohammad Ali Khan, Yin Yang, Hassan Sajjad, Preslav Nakov, Deming Chen, and Marianne Winslett. Compressing large-scale transformer-based models: A case study on bert. *Transactions of the Association for Computational Linguistics*, 9:1061–1080, 2021.\n",
      "\n",
      "[5] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. A survey of large language models. *arXiv preprint arXiv:2303.18223*, 2023.\n",
      "\n",
      "[6] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. *arXiv preprint arXiv:2106.09685*, 2021.\n",
      "\n",
      "[7] Ian J Goodfellow, Mehdi Mirza, Da Xiao, Aaron Courville, and Yoshua Bengio. An empirical investigation of catastrophic forgetting in gradient-based neural networks. *arXiv preprint arXiv:1312.6211*, 2013.\n",
      "\n",
      "[8] Nan Jiang, Kevin Liu, Thibaud Lutellier, and Lin Tan. Impact of code language models on automated program repair. *In 2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE)*, pages 1430-1442. IEEE, 2023.\n",
      "\n",
      "[9] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. *arXiv preprint arXiv:1503.02531*, 2015.\n",
      "\n",
      "[10] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. *arXiv preprint arXiv:1701.06538*, 2017.\n",
      "\n",
      "[11] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter. *arXiv preprint arXiv:1910.01108*, 2019.\n",
      "\n",
      "[12] Yuxian Gu, Li Dong, Furu Wei, and Minlie Huang. Minillm: Knowledge distillation of large language models. *In The Twelfth International Conference on Learning Representations*, 2023.\n",
      "\n",
      "[13] Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. Mixtral of experts. *arXiv preprint arXiv:2401.04088*, 2024.\n",
      "\n",
      "[14] Sainbayar Sukhbaatar, Olga Golovneva, Vasu Sharma, Hu Xu, Xi Victoria Lin, Baptiste Rozière, Jacob Kahn, Daniel Li, Wen-tau Yih, Jason Weston, et al. Branch-train-mix: Mixing expert llms into a mixture-of-experts Ilm. *arXiv preprint arXiv:2403.07816*, 2024.\n",
      "\n",
      "[15] Margaret Li, Suchin Gururangan, Tim Dettmers, Mike Lewis, Tim Althoff, Noah A Smith, and Luke Zettlemoyer. Branch-train-merge: Embarrassingly parallel training of expert language models. *arXiv preprint arXiv:2208.03306*, 2022.\n",
      "\n",
      "[16] Mandy Guo, Zihang Dai, Denny Vrandečić, and Rami Al-Rfou. Wiki-40b: Multilingual language model dataset. *In Proceedings of the Twelfth Language Resources and Evaluation Conference*, pages 2440-2452, 2020.\n",
      "\n",
      "[17] CodeParrot. github-code-clean dataset. https://huggingface.co/datasets/codeparrot/github-code-clean, n.d. Accessed: 2024-06-21.\n",
      "\n",
      "[18] Philip Gage. A new algorithm for data compression. *The C Users Journal*, 12(2):23–38, 1994.\n",
      "\n",
      "---\n",
      "\n",
      "[^1]: https://zenodo.org/doi/10.5281/zenodo.12677631\n",
      "[^2]: https://github.com/padas-lab-de/multi-language-dataset-creator\n",
      "[^3]: https://github.com/ModMaamari/mixture-modular-experts\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "response = client.models.generate_content(\n",
    "  model=models[1],\n",
    "  contents=[\n",
    "      types.Part.from_bytes(\n",
    "        data=filepath.read_bytes(),\n",
    "        mime_type='application/pdf',\n",
    "      ),\n",
    "      promptv1])\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "promptv2 = \"\"\"You are given a scan of a PDF file. Convert the PDF to Markdown.\n",
    "When you encounter an image, return its bounding box in [ymin, xmin, ymax, xmax] format. You should return the bounding box for EVERY image in the document. Surround the bounding box in <bounding_box></bounding_box> tags. Inside these tags, return the information in ([ymin, xmin, ymax, xmax], page_number) format\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```markdown\n",
      "arXiv:2407.19610v1 [cs.AI] 28 Jul 2024\n",
      "\n",
      "# MIXTURE OF MODULAR EXPERTS: DISTILLING KNOWLEDGE FROM A MULTILINGUAL TEACHER INTO SPECIALIZED MODULAR LANGUAGE MODELS\n",
      "\n",
      "**Mohammed Al-Maamari**\n",
      "Chair of Data Science\n",
      "University of Passau\n",
      "Germany, Passau\n",
      "Mohammed.Al-Maamari@uni-passau.de\n",
      "\n",
      "**Mehdi Ben Amor**\n",
      "Chair of Data Science\n",
      "University of Passau\n",
      "Germany, Passau\n",
      "Mehdi.BenAmor@uni-passau.de\n",
      "\n",
      "**Michael Granitzer**\n",
      "Chair of Data Science\n",
      "University of Passau\n",
      "Germany, Passau\n",
      "Michael.Granitzer@uni-passau.de\n",
      "\n",
      "July 30, 2024\n",
      "\n",
      "## ABSTRACT\n",
      "\n",
      "This research explores the integration of Knowledge Distillation (KD) and Mixture of Experts (MoE) to create modular, efficient, and specialized multilingual language models. The primary objectives include evaluating adaptive versus fixed alpha methods in KD, developing and comparing modular MoE architectures in handling multi-domain inputs and preventing catastrophic forgetting.\n",
      "\n",
      "We address the computational challenges of large language models (LLMs) and the need for modular models. KD compresses LLMs into smaller models for efficiency, while MoE architectures enhance modularity by combining multiple experts for specialized tasks. Experiments showed that adaptive and fixed alpha methods in KD yielded similar performance, with marginal improvements from adaptive alpha. The combined loss approach slightly outperformed alternating losses, providing more stable learning. The router, trained to classify input sequences into English, French, German, or Python, achieved high accuracy precision, recall, and F1 score of 99.95%, with Logistic Regression as the most effective classifier.\n",
      "\n",
      "Evaluation of modular MoE architectures revealed that the Pre-trained Language Experts (PLE) setup and Joint Expert Embedding Training (JEET) demonstrated similar performance, while the MoE with Common Expert (MoE-CE) setup showed slightly lower performance. However, when including a common expert in MoE-CE, its performance approaches that of both PLE and JEET. The study on catastrophic forgetting indicated that sequential training led to significant forgetting, while single-session training with balanced batches approach, and MoE approach mitigated this issue effectively. The MoE architecture preserved knowledge across multiple languages, demonstrating its\n",
      "\n",
      "A PREPRINT JULY 30, 2024\n",
      "\n",
      "effectiveness. We open-sourced the dataset ¹, the balanced dataset creation tool ², and the research codebase ³.\n",
      "\n",
      "## 1 Introduction\n",
      "\n",
      "Language models (LMs) are pivotal in Natural Language Processing (NLP), facilitating a variety of tasks such as machine translation [1], sentiment analysis [2], and text generation [3]. Despite their potential, large-scale models encounter challenges like computational inefficiency, limited adaptability, and catastrophic forgetting. Our study explores the amalgamation of Knowledge Distillation (KD) and Mixture of Experts (MoE) to mitigate these challenges, aiming to improve efficiency, modularity, and specialization in language models.\n",
      "\n",
      "Transformers, the backbone of many large models, require substantial computational resources [4], which hampers their scalability and accessibility. The increasing complexity and size associated with supporting more languages and domains adversely affect training durations and generalization abilities [5]. Additionally, fine-tuning for specific tasks consumes significant resources and often falls short of achieving optimal outcomes [6]. Catastrophic forgetting is a major hurdle, particularly in models handling multiple languages and domains, as they tend to lose previously acquired knowledge when exposed to new data [7].\n",
      "\n",
      "Specialized models, when trained on narrow domains such as programming languages, have demonstrated superiority in specific tasks like code completion and bug detection over their general-purpose counterparts [8]. Introducing modularity into neural network design enhances flexibility, scalability, and maintainability, enabling updates to individual network segments without necessitating a complete retraining.\n",
      "\n",
      "This research primarily focuses on exploring various integration strategies of KD and MoE to create specialized, efficient, and modular language models. While we employed straightforward knowledge distillation techniques, reaching state-of-the-art knowledge distillation was not our objective. Instead, our primary goal was to investigate the feasibility of different integration methods of KD and MoE. KD is the process where smaller student models learn to mimic the behavior of a larger, more capable teacher model using their probabilistic outputs [9]. MoE architectures, on the other hand, dynamically delegate tasks to specialized models, thereby enhancing performance across varied domains and languages [10].\n",
      "\n",
      "Our research objectives include evaluating adaptive versus fixed alpha methods in KD, training a router to efficiently direct inputs to the appropriate experts, and comparing various MoE architectures to determine their effectiveness in handling multi-domain inputs and in averting catastrophic forgetting. This study contributes to the development of more adept and effective NLP systems that can support a broad spectrum of applications.\n",
      "\n",
      "## 2 Related Work\n",
      "\n",
      "### 2.1 Knowledge Distillation\n",
      "\n",
      "\"DistilBERT\" by Sanh et al. [11] introduces a method to distill BERT into a smaller, faster model while retaining most of its performance. By leveraging a triple loss function (language modeling, distillation, and cosine-distance losses), DistilBERT reduces model size by 40% and maintains 97% of BERT's language understanding capabilities. This approach makes the model suitable for deployment in environments with constrained computational resources. DistilBERT's training involves using every second layer from the teacher model to retain inductive biases and employs techniques like gradient accumulation and dynamic masking.\n",
      "\n",
      "¹https://zenodo.org/doi/10.5281/zenodo.12677631\n",
      "²https://github.com/padas-lab-de/multi-language-dataset-creator\n",
      "³https://github.com/ModMaamari/mixture-modular-experts\n",
      "\n",
      "A PREPRINT JULY 30, 2024\n",
      "\n",
      "\"MiniLLM\" by Gu et al. [12] proposes using reverse Kullback-Leibler divergence (KLD) for knowledge distillation to address the issue of overestimating low-probability regions in the teacher model's distribution. The method minimizes the difference between the teacher and student model distributions using a conditional text generation framework. Techniques such as single-step decomposition and teacher-mixed sampling are employed to stabilize and accelerate the training process. Our work adopts a simplified version of reverse KLD at the word level, focusing on efficiently capturing the teacher model's probabilistic characteristics.\n",
      "\n",
      "### 2.2 Mixture of Experts\n",
      "\n",
      "\"Mixtral of Experts\" by Jiang et al. [13] presents Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model. The model dynamically selects two out of eight feedforward blocks per token at each layer, optimizing computational resource usage. Mixtral's transformer model incorporates MoE layers with a routing mechanism to allocate tokens to experts. Evaluations show superior performance in various benchmarks, highlighting the model's efficiency and effectiveness.\n",
      "\n",
      "\"Branch-Train-MiX\" by Sukhbaatar et al. [14] investigates methods for training LLMs across multiple specialized domains. The Branch-Train-MiX (BTX) method involves branching from a seed model, training domain-specific experts, and integrating them into a unified MoE model. This approach improves training efficiency and model performance by leveraging parallelism and specialization. BTX outperforms baselines like Llama-2 in accuracy and computational efficiency.\n",
      "\n",
      "\"Branch-Train-Merge\" by Li et al. [15] introduces the Branch-Train-Merge (BTM) algorithm, which enhances the efficiency of training large language models. BTM facilitates independent training of subparts of the model on different data subsets, reducing communication overhead. The approach involves three steps: branching, training, and merging. BTM achieves improved perplexities and higher updates per second due to reduced communication overhead, making it a scalable and efficient training paradigm.\n",
      "\n",
      "Our research integrates MoE with Knowledge Distillation (KD) to develop specialized multilingual models. Unlike Mixtral and BTX, which focus on token-level routing and parallel training of domain-specific experts, our work emphasizes sequence-level routing and the integration of KD with MoE. This approach aims to address multi-domain adaptability and reduce catastrophic forgetting, contributing to the development of modular and efficient language models.\n",
      "\n",
      "## 3 Methods\n",
      "\n",
      "This section describes the methodologies, tools, and algorithms used in this research, focusing on dataset preparation, model training, knowledge distillation techniques, and MoE architecture design.\n",
      "\n",
      "### 3.1 Dataset Preparation\n",
      "\n",
      "The dataset comprises multilingual text data in English, German, French, and Python code. The primary sources are the Wiki40B dataset [16] for natural languages and the CodeParrot GitHub \"codeparrot/github-code-clean\" dataset [17] for Python code. The Wiki40B dataset includes 2,926,536 English, 1,227,206 French, and 1,554,908 German training samples, ensuring balanced token counts across these languages. Python data contains 645K Python code files, filtered and balanced based on code length to ensure comprehensive coverage.\n",
      "\n",
      "A PREPRINT JULY 30, 2024\n",
      "\n",
      "**Table 1: Number of Articles per Language**\n",
      "| Language | Training Samples | Validation Samples |\n",
      "|---|---|---|\n",
      "| English | 2,926,536 | 163,597 |\n",
      "| French | 1,227,206 | 68,655 |\n",
      "| German | 1,554,908 | 86,068 |\n",
      "\n",
      "### 3.2 Tokenization\n",
      "\n",
      "Byte Pair Encoding (BPE) [18] was used for tokenization, with a vocabulary size of 32,000 tokens. The tokenizer was trained on a balanced dataset of the four languages, using special tokens such as `<unk>`, `<s>`, and `</s>`. This setup ensured efficient vocabulary usage and compatibility with the models.\n",
      "\n",
      "<bounding_box>([306, 145, 629, 849], 4)</bounding_box>\n",
      "**Figure 1: Dataset Splits for Each Language**\n",
      "\n",
      "### 3.3 Teacher Model Training\n",
      "\n",
      "The teacher model, a GPT-2 Medium with 340 million parameters, was trained on the multilingual dataset. Initial attempts with Mistral 1.6B and Phi 1.34B configurations faced stability issues, leading to the selection of GPT-2 Medium for its balance of performance and computational efficiency. The training process involved a context length of 1024 tokens, a virtual batch size of 512, and optimization techniques such as gradient accumulation and clipping. Training was conducted on 2 A100 GPUs using DeepSpeed and Accelerate libraries.\n",
      "\n",
      "### 3.4 Knowledge Distillation\n",
      "\n",
      "Knowledge distillation (KD) involved transferring knowledge from the GPT-2 Medium teacher model to smaller student models. The student models were trained to replicate the teacher's outputs using a combined loss function incorporating Cross-Entropy Loss and Reverse Kullback-Leibler (RKL) Divergence Loss [12]. The adaptive alpha method dynamically adjusted the weights of these losses based on training progress.\n",
      "\n",
      "A PREPRINT JULY 30, 2024\n",
      "<bounding_box>([138, 175, 472, 817], 5)</bounding_box>\n",
      "**Figure 2: Learning Curve for Mistral 1.6B, Mistral GPT-M 440M, Phi 1.34B**\n",
      "\n",
      "```\n",
      "L_total = α · L_LM + β · L_KD (1)\n",
      "```\n",
      "\n",
      "### 3.5 Mixture of Experts Architecture\n",
      "\n",
      "The Mixture of Experts (MoE) architecture includes multiple specialized sub-models (experts) and a router mechanism. Three setups were evaluated:\n",
      "\n",
      "*   **Setup 1: Pre-trained Language Experts (PLE)** - Experts pre-trained independently via KD from the teacher model, handling specific languages.\n",
      "*   **Setup 2: Joint Expert Embedding Training (JEET)** - Experts trained concurrently with a shared embedding layer, maintaining modularity.\n",
      "*   **Setup 3: MoE with Common Expert (MoE-CE)** - Includes a common expert trained on all languages, sharing the embedding layer with specialized experts.\n",
      "\n",
      "### 3.6 Router\n",
      "\n",
      "The router, pre-trained to classify inputs into English, French, German, or Python, achieved high accuracy (99.95%) using TF-IDF vectorization and Logistic Regression. It dynamically selects the appropriate expert during inference, optimizing the allocation of computational resources.\n",
      "\n",
      "A PREPRINT JULY 30, 2024\n",
      "<bounding_box>([103, 118, 380, 879], 6)</bounding_box>\n",
      "**Figure 3: Our Knowledge Distillation Process**\n",
      "\n",
      "<bounding_box>([408, 183, 627, 811], 6)</bounding_box>\n",
      "**Figure 4: Architecture of the Joint Expert Embedding Training MoE Setup**\n",
      "\n",
      "### 3.7 Training and Inference\n",
      "\n",
      "During training, data was batched by language, and the router directed each batch to the corresponding expert. This approach ensured specialization without interference. During inference, the system handled mixed-language batches, maintaining efficiency and specialization.\n",
      "\n",
      "**Table 2: Model Configurations and Hyperparameters**\n",
      "| Model | Parameters | Configuration Source | Hyperparameters |\n",
      "|---|---|---|---|\n",
      "| Mistral 1.6B | 1.6B | Phi1.5 | Custom |\n",
      "| Phi 1.34B | 1.34B | Phi1.5 | Standard |\n",
      "| GPT-2 110M | 110M | GPT-2 | Standard |\n",
      "| GPT-2 340M | 340M | GPT-2 | Standard |\n",
      "| Distil-GPT2 | 82M | Distil-GPT2 | Standard |\n",
      "\n",
      "A PREPRINT JULY 30, 2024\n",
      "<bounding_box>([103, 184, 330, 811], 7)</bounding_box>\n",
      "**Figure 5: Architecture of the MoE with Common Expert Setup**\n",
      "\n",
      "<bounding_box>([364, 184, 596, 811], 7)</bounding_box>\n",
      "**Figure 6: Architecture of the Pre-trained Language Experts**\n",
      "\n",
      "## 4 Results\n",
      "\n",
      "The experiments focused on evaluating different aspects of the Mixture of Experts (MoE) architecture, including Knowledge Distillation (KD) methods, router training, and the impact of modular MoE architectures on system performance.\n",
      "\n",
      "### 4.1 Experimental Setup\n",
      "\n",
      "The experimental setup involved two distinct KD approaches: independent KD and sequential KD. Independent KD trained separate student models for each language (English, French, German, and Python) using the outputs of the teacher model. Sequential KD distilled a single student model sequentially with knowledge from each language. The evaluation metrics used were perplexity and cross-entropy loss. The training hyperparameters, including a context length of 1024 tokens and a vocabulary size of 32,000, were consistent with those used for the teacher model. The MoE training utilized the same tokenizer and frameworks such as DeepSpeed and Accelerate to manage computational demands.\n",
      "\n",
      "A PREPRINT JULY 30, 2024\n",
      "\n",
      "### 4.2 Adaptive vs. Fixed Alpha\n",
      "\n",
      "**Research Question 1 (RQ1):** *What is the effectiveness of adaptive vs. fixed alpha methods in Knowledge Distillation?*\n",
      "\n",
      "The adaptive alpha method, which dynamically adjusts weights during training, was hypothesized to perform better than the fixed alpha method. The results, shown in Figure 7, indicated that the adaptive alpha method outperformed the fixed alpha method by a small margin (0.01 improvement in evaluation loss). Testing different fixed alpha values showed that an alpha of 0.5 performed nearly as well as the adaptive method, suggesting minimal benefits of dynamic adjustment in this setup.\n",
      "\n",
      "<bounding_box>([250, 229, 588, 795], 8)</bounding_box>\n",
      "**Figure 7: Evaluation loss for Adaptive Alpha and different values of Fixed Alpha**\n",
      "\n",
      "### 4.3 Alternating Losses (AL) VS Combined Losses (CL)\n",
      "\n",
      "**Research Question 2 (RQ2):** *What is the impact of alternating losses during training on model convergence and performance?*\n",
      "\n",
      "The hypothesis was that alternating KD_loss and LM_loss would enhance model performance. However, the results, shown in Figure 8, indicated that the combined loss method performed slightly better, with an evaluation loss marginally lower than the alternating losses method (4.305 vs. 4.322). This suggests that consistently applying both losses at each training step provides a more effective learning signal.\n",
      "\n",
      "### 4.4 The Router\n",
      "\n",
      "**Research Question 3 (RQ3):** *Can we train a router that accurately routes input sequences to one of the four experts, classifying the input sequence into one of the four classes [En, Fr, De, Py]?*\n",
      "\n",
      "Various classifiers were evaluated, with the Logistic Regression classifier achieving the highest performance (99.95% accuracy). The confusion matrix in Figure 9 and Table 3 confirmed the router's effectiveness in accurately classifying input sequences.\n",
      "\n",
      "A PREPRINT JULY 30, 2024\n",
      "<bounding_box>([120, 229, 402, 763], 9)</bounding_box>\n",
      "**Figure 8: Evaluation Loss for Combined Loss vs. Alternating Losses Methods**\n",
      "\n",
      "**Table 3: Performance Metrics of Different Classifiers for Router Training**\n",
      "| Classifier | Accuracy ↑ | Precision ↑ | Recall ↑ | F1 Score ↑ |\n",
      "|---|---|---|---|---|\n",
      "| Logistic Regression | 0.9995 | 0.9995 | 0.9995 | 0.9995 |\n",
      "| SGD Classifier | 0.9993 | 0.9993 | 0.9993 | 0.9993 |\n",
      "| Random Forest | 0.9995 | 0.9995 | 0.9995 | 0.9995 |\n",
      "\n",
      "### 4.5 Modular MoE Model Design\n",
      "\n",
      "**Research Question 4 (RQ4):** *How do the three MoE architectures compare in terms of performance and robustness?*\n",
      "\n",
      "Three MoE setups were compared: Pre-trained Language Experts (PLE), Joint Expert Embedding Training (JEET), and MoE with Common Expert (MoE-CE). The results, summarized in Table 4, showed that PLE achieved the best perplexities for English and German, while JEET performed best for French and Python. MoE-CE, when using the common expert, showed performance approaching that of PLE and JEET, highlighting the benefits of including a common expert.\n",
      "\n",
      "**Table 4: Perplexity Metrics for Different MoE Architectures**\n",
      "| Architecture | English ↓ | French ↓ | German ↓ | Python ↓ |\n",
      "|---|---|---|---|---|\n",
      "| PLE | **74.09** | 20.30 | **39.86** | 28.92 |\n",
      "| JEET | 75.79 | **20.12** | 40.38 | **27.02** |\n",
      "| MoE-CE w/o CE | 90.83 | 23.24 | 47.75 | 29.89 |\n",
      "| MoE-CE + CE | 78.96 | 20.91 | 41.92 | 27.16 |\n",
      "\n",
      "### 4.6 Impact of Adding a Common Expert to the MoE System\n",
      "\n",
      "**Research Question 5 (RQ5):** *Does adding a common expert improve the overall performance of the MoE system and the performance of each expert independently?*\n",
      "\n",
      "A PREPRINT JULY 30, 2024\n",
      "<bounding_box>([134, 234, 465, 761], 10)</bounding_box>\n",
      "**Figure 9: Confusion Matrix for the Router**\n",
      "\n",
      "The addition of a common expert in MoE-CE generally improved performance, as shown in Table 5. The overall perplexity was reduced by 5.69 points with the inclusion of the common expert, confirming its positive impact.\n",
      "\n",
      "### 4.7 Catastrophic Forgetting in Modular MoE Architecture VS Non-Modular Approaches\n",
      "\n",
      "**Research Question 6 (RQ6):** *How does the modular MoE architecture compare to non-modular approaches in terms of catastrophic forgetting?*\n",
      "\n",
      "Three experiments were conducted to compare the impact of catastrophic forgetting: 1. Sequentially distilling knowledge into a single student model. 2. Distilling knowledge into a single student model in one session. 3. Employing the MoE architecture with four separate student models.\n",
      "\n",
      "The results, shown in Table 6, indicated significant catastrophic forgetting in the sequential setup (up to 38% in German), whereas both single session distillation and MoE showed no catastrophic forgetting. The evaluation loss comparison for sequential training and MoE architecture is shown in Figure 10.\n",
      "\n",
      "The results confirmed that the modular MoE architecture effectively mitigates catastrophic forgetting, supporting the hypothesis that modularity allows for the retention of knowledge across multiple languages.\n",
      "\n",
      "## 5 Discussion\n",
      "\n",
      "This section interprets and contextualizes the findings from our experiments, providing insights into the efficacy of different Knowledge Distillation (KD) methods and Mixture of Experts (MoE) architectures. It addresses the research questions, compares our results with existing literature, and reflects on the challenges and limitations encountered during the research.\n",
      "\n",
      "A PREPRINT JULY 30, 2024\n",
      "\n",
      "**Table 5: Perplexity scores for Different Inference Settings in MoE-CE**\n",
      "| CommonExpert | RoutableExperts | En ↓ | Fr ↓ | De ↓ | Py ↓ | All ↓ |\n",
      "|---|---|---|---|---|---|---|\n",
      "| ✓ | En, Fr, De, Py | **78.96** | **20.91** | **41.92** | **27.16** | **42.24** |\n",
      "| X | En, Fr, De, Py | 90.83 | 23.24 | 47.75 | 29.89 | 47.93 |\n",
      "| ✓ | En, Py | 78.96 | 36.55 | 79.95 | 27.16 | 55.65 |\n",
      "| X | En, Py | 90.83 | 116.04 | 325.28 | 29.89 | 140.51 |\n",
      "| ✓ | Fr, Py | 139.09 | 20.91 | 77.64 | 27.16 | 66.20 |\n",
      "| X | Fr, Py | 534.64 | 23.24 | 305.62 | 29.89 | 223.35 |\n",
      "| ✓ | De, Py | 140.33 | 37.10 | 41.92 | 27.16 | 61.63 |\n",
      "| X | De, Py | 532.25 | 124.18 | 47.75 | 29.89 | 183.52 |\n",
      "| ✓ | En | 78.96 | 36.55 | 79.95 | 53.65 | 62.28 |\n",
      "| X | En | 90.83 | 116.04 | 325.28 | 203.50 | 183.91 |\n",
      "| ✓ | Fr | 139.09 | 20.91 | 77.64 | 68.62 | 76.56 |\n",
      "| X | Fr | 534.64 | 23.24 | 305.62 | 577.22 | 360.18 |\n",
      "| ✓ | De | 140.33 | 37.10 | 41.92 | 55.11 | 68.62 |\n",
      "| X | De | 532.25 | 124.18 | 47.75 | 240.68 | 236.22 |\n",
      "| ✓ | Py | 113.47 | 38.18 | 80.44 | 27.16 | 64.81 |\n",
      "| X | Py | 284.49 | 141.93 | 379.81 | 29.89 | 209.03 |\n",
      "| ✓ | None | 104.01 | 27.66 | 57.72 | 38.03 | 56.86 |\n",
      "\n",
      "**Table 6: Forgotten Knowledge in Different Experiments**\n",
      "| Experiment | Language | Forgotten Knowledge ↓ |\n",
      "|---|---|---|\n",
      "|  | English | 0.499 (12.0%) |\n",
      "| A (Sequential) | French | 0.851 (31.0%) |\n",
      "|  | German | 1.301 (38.0%) |\n",
      "|  | Python | N/A |\n",
      "|  | English | 0 (0%) |\n",
      "| B (Single Session) | French | 0 (0%) |\n",
      "|  | German | 0 (0%) |\n",
      "|  | Python | 0 (0%) |\n",
      "|  | English | 0 (0%) |\n",
      "| C (MoE) | French | 0 (0%) |\n",
      "|  | German | 0 (0%) |\n",
      "|  | Python | 0 (0%) |\n",
      "\n",
      "### 5.1 Interpretation of Findings\n",
      "\n",
      "#### 5.1.1 Adaptive vs. Fixed Alpha\n",
      "\n",
      "The comparison between adaptive alpha and fixed alpha methods, discussed in subsection 4.2, revealed that both approaches yielded similar performance levels. The adaptive alpha approach showed a marginally better performance, but this advantage was minimal due to the consistency of the dataset used for training both the teacher and student models. The similarity in KD loss and cross-entropy loss changes likely contributed to the comparable performance of both methods.\n",
      "\n",
      "A PREPRINT JULY 30, 2024\n",
      "<bounding_box>([114, 241, 389, 795], 12)</bounding_box>\n",
      "**Figure 10: Evaluation Loss Comparison for Sequential Training (Experiment A) and MoE Architecture (Experiment C)**\n",
      "\n",
      "#### 5.1.2 Alternating Losses vs. Combined Losses\n",
      "\n",
      "The experiment comparing alternating losses (AL) and combined losses (CL), as described in subsection 4.3, indicated that the combined loss approach slightly outperformed the alternating loss approach. The minimal difference suggests that the simplicity of the combination method—weighted averaging of the losses—may have masked potential benefits of alternating losses. More sophisticated methods of loss alternation may yield more pronounced differences and warrant further exploration.\n",
      "\n",
      "#### 5.1.3 Router Performance\n",
      "\n",
      "The router's performance, detailed in subsection 4.4, achieved high accuracy, precision, recall, and F1 scores. The distinctiveness of the four classes (English, German, French, and Python) and the balanced dataset used for training contributed to this success. The results confirm the router's capability to accurately classify input sequences and support the MoE architecture's performance.\n",
      "\n",
      "#### 5.1.4 Modular MoE Model Design\n",
      "\n",
      "The performance comparison of the three MoE architectures, discussed in subsection 4.5, highlighted the strengths and limitations of each setup. Pre-trained Language Experts (PLE) and Joint Expert Embedding Training (JEET) performed comparably, with PLE excelling in English and German, and JEET in French and Python. MoE with Common Expert (MoE-CE) improved significantly with the inclusion of a common expert during inference, suggesting that a shared knowledge base can enhance performance in multi-language tasks.\n",
      "\n",
      "#### 5.1.5 Catastrophic Forgetting in Modular vs. Non-Modular Approaches\n",
      "\n",
      "The study on catastrophic forgetting, as outlined in subsection 4.7, demonstrated that sequential training led to significant forgetting of previously learned languages, while single-session training and MoE approaches effectively mitigated\n",
      "\n",
      "A PREPRINT JULY 30, 2024\n",
      "\n",
      "this issue. The MoE approach, which assigns dedicated experts to each language, completely eliminated catastrophic forgetting, highlighting the effectiveness of modular architectures in preserving knowledge across multiple domains.\n",
      "\n",
      "### 5.2 Comparison with Existing Literature\n",
      "\n",
      "Our use of reverse Kullback-Leibler divergence (RKL) for knowledge distillation aligns with the approach presented in \"MiniLLM: Knowledge Distillation of Large Language Models\" [12], which demonstrated improved alignment between student and teacher models. Our findings validate the efficacy of RKL in our multilingual and modular model context.\n",
      "\n",
      "In comparison to \"Mixtral of Experts\" [13], which uses a sparse MoE architecture, our approach emphasizes modularity and specialization. Our method allows for the utilization of any subset of experts without compromising their individual performance and includes a common expert to enhance performance across languages.\n",
      "\n",
      "Our research also contrasts with \"Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM\" [14], which focuses on parallel training of domain-specific experts. Our approach combines KD and MoE, resulting in smaller, efficient student experts that maintain high performance. This dual focus on modularity and efficiency distinguishes our research.\n",
      "\n",
      "### 5.3 Challenges and Limitations\n",
      "\n",
      "#### 5.3.1 Challenges\n",
      "\n",
      "One significant challenge was the computational resource constraints, which necessitated the use of techniques such as gradient accumulation and efficient memory management. Ensuring balanced representation of the dataset also posed a challenge, requiring meticulous preprocessing and cleaning.\n",
      "\n",
      "#### 5.3.2 Limitations\n",
      "\n",
      "The primary limitation is the scale of the dataset. Our training dataset size of 490 million tokens is considerably smaller than those used in training state-of-the-art language models. This limitation affects the generalizability of our findings to larger-scale applications. Additionally, the focus on a limited number of languages and a single programming language (Python) restricts the applicability of our findings.\n",
      "\n",
      "#### 5.3.3 Future Work\n",
      "\n",
      "Future work should focus on addressing these limitations and exploring the scalability of our approach to larger datasets and more diverse languages and domains. Further research on the adaptive alpha method could help refine its implementation and identify scenarios where its benefits are more pronounced. Optimizing the training and integration process for the MoE architecture and investigating its applicability to other languages and domains will also be crucial.\n",
      "\n",
      "## 6 Conclusion\n",
      "\n",
      "### 6.1 Summary of Contributions\n",
      "\n",
      "This research integrates Knowledge Distillation (KD) and Mixture of Experts (MoE) to develop modular, efficient, and specialized multilingual language models. The primary objectives were to evaluate adaptive versus fixed alpha methods in KD, compare modular MoE architectures, and address catastrophic forgetting.\n",
      "\n",
      "A PREPRINT JULY 30, 2024\n",
      "\n",
      "#### 6.1.1 Knowledge Distillation\n",
      "\n",
      "The experiments comparing adaptive and fixed alpha methods in KD revealed similar performance, with the adaptive alpha method providing a slight improvement. The combined loss approach offered more stable learning dynamics compared to alternating losses.\n",
      "\n",
      "#### 6.1.2 Mixture of Experts\n",
      "\n",
      "Three MoE architectures were assessed: Pre-trained Language Experts (PLE), Joint Expert Embedding Training (JEET), and MoE with Common Expert (MoE-CE). PLE and JEET performed similarly, while MoE-CE, without utilizing the common expert, lagged behind but demonstrated enhanced results with the inclusion of a common expert. This indicates the effectiveness of shared knowledge in improving performance across multiple languages.\n",
      "\n",
      "#### 6.1.3 Router Performance\n",
      "\n",
      "The router, employing Logistic Regression for classification, achieved high accuracy and reliability in selecting the appropriate expert model for inputs, with accuracy, recall, precision, and F1-score all at 99.95%.\n",
      "\n",
      "#### 6.1.4 Catastrophic Forgetting\n",
      "\n",
      "Sequential training resulted in significant catastrophic forgetting, whereas single-session training and the MoE approach effectively mitigated this issue. The modular MoE architecture preserved knowledge across multiple languages, preventing catastrophic forgetting.\n",
      "\n",
      "### 6.2 Implications and Impact\n",
      "\n",
      "The integration of KD with MoE facilitates the development of modular, specialized, and efficient models that perform well across diverse tasks. The modularity of the MoE architecture enhances flexibility, allowing for the addition of new experts without retraining the entire system and addressing catastrophic forgetting by enabling the model to retain knowledge across multiple languages and domains.\n",
      "\n",
      "### 6.3 Challenges and Limitations\n",
      "\n",
      "The research was constrained by computational resources and a relatively small dataset of 490 million tokens, limiting the generalizability of the findings. Additionally, the focus on a limited number of languages and a single programming language indicates that further experimentation is needed to extend the approach to other languages, domains, and modalities.\n",
      "\n",
      "### 6.4 Future Work\n",
      "\n",
      "Future research should aim to scale the approach to larger datasets and more diverse languages and domains. Optimizing the training and integration process for the MoE architecture and exploring the applicability of the methods to other contexts are recommended. Further investigation into adaptive alpha methods and advanced loss functions for the common expert could provide deeper insights and enhance model performance.\n",
      "\n",
      "## References\n",
      "\n",
      "[1] Gennadi Lembersky, Noam Ordan, and Shuly Wintner. Language models for machine translation: Original vs. translated texts. *Computational Linguistics*, 38(4):799–825, 2012.\n",
      "\n",
      "A PREPRINT JULY 30, 2024\n",
      "\n",
      "[2] Wenxuan Zhang, Yue Deng, Bing Liu, Sinno Jialin Pan, and Lidong Bing. Sentiment analysis in the era of large language models: A reality check. *arXiv preprint arXiv:2305.15005*, 2023.\n",
      "[3] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. *Advances in neural information processing systems*, 33:1877–1901, 2020.\n",
      "[4] Prakhar Ganesh, Yao Chen, Xin Lou, Mohammad Ali Khan, Yin Yang, Hassan Sajjad, Preslav Nakov, Deming Chen, and Marianne Winslett. Compressing large-scale transformer-based models: A case study on bert. *Transactions of the Association for Computational Linguistics*, 9:1061–1080, 2021.\n",
      "[5] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. A survey of large language models. *arXiv preprint arXiv:2303.18223*, 2023.\n",
      "[6] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. *arXiv preprint arXiv:2106.09685*, 2021.\n",
      "[7] Ian J Goodfellow, Mehdi Mirza, Da Xiao, Aaron Courville, and Yoshua Bengio. An empirical investigation of catastrophic forgetting in gradient-based neural networks. *arXiv preprint arXiv:1312.6211*, 2013.\n",
      "[8] Nan Jiang, Kevin Liu, Thibaud Lutellier, and Lin Tan. Impact of code language models on automated program repair. In *2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE)*, pages 1430–1442. IEEE, 2023.\n",
      "[9] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. *arXiv preprint arXiv:1503.02531*, 2015.\n",
      "[10] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. *arXiv preprint arXiv:1701.06538*, 2017.\n",
      "[11] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter. *arXiv preprint arXiv:1910.01108*, 2019.\n",
      "[12] Yuxian Gu, Li Dong, Furu Wei, and Minlie Huang. Minillm: Knowledge distillation of large language models. In *The Twelfth International Conference on Learning Representations*, 2023.\n",
      "[13] Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. Mixtral of experts. *arXiv preprint arXiv:2401.04088*, 2024.\n",
      "[14] Sainbayar Sukhbaatar, Olga Golovneva, Vasu Sharma, Hu Xu, Xi Victoria Lin, Baptiste Rozière, Jacob Kahn, Daniel Li, Wen-tau Yih, Jason Weston, et al. Branch-train-mix: Mixing expert llms into a mixture-of-experts llm. *arXiv preprint arXiv:2403.07816*, 2024.\n",
      "[15] Margaret Li, Suchin Gururangan, Tim Dettmers, Mike Lewis, Tim Althoff, Noah A Smith, and Luke Zettlemoyer. Branch-train-merge: Embarrassingly parallel training of expert language models. *arXiv preprint arXiv:2208.03306*, 2022.\n",
      "[16] Mandy Guo, Zihang Dai, Denny Vrandečić, and Rami Al-Rfou. Wiki-40b: Multilingual language model dataset. In *Proceedings of the Twelfth Language Resources and Evaluation Conference*, pages 2440–2452, 2020.\n",
      "[17] CodeParrot. github-code-clean dataset. https://huggingface.co/datasets/codeparrot/ github-code-clean, n.d. Accessed: 2024-06-21.\n",
      "[18] Philip Gage. A new algorithm for data compression. *The C Users Journal*, 12(2):23–38, 1994.\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "response = client.models.generate_content(\n",
    "  model=models[1],\n",
    "  contents=[\n",
    "      types.Part.from_bytes(\n",
    "        data=filepath.read_bytes(),\n",
    "        mime_type='application/pdf',\n",
    "      ),\n",
    "      promptv2])\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Satisfactory Results? Let's Polish Them with Images\n",
    "- save_to_markdown\n",
    "- parse_markdown\n",
    "- parse_out_markdown_tildas\n",
    "- pdf_page_to_image\n",
    "- parse_bbs\n",
    "- extract_images\n",
    "- etract_all_images\n",
    "- save_extracted_images\n",
    "- insert_images_into_markdown\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_markdown(content, filename=\"output.md\"):\n",
    "    \"\"\"\n",
    "    Saves a string to a markdown file.\n",
    "    \n",
    "    Args:\n",
    "        content (str): The content to save to the markdown file.\n",
    "        filename (str, optional): The name of the file to save to. Defaults to \"output.md\".\n",
    "    \n",
    "    Returns:\n",
    "        pathlib.Path: The path to the saved file.\n",
    "    \"\"\"\n",
    "    file_path = pathlib.Path(filename)\n",
    "    \n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(content)\n",
    "    \n",
    "    return file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_markdown(text):\n",
    "    \"\"\"\n",
    "    Extracts content inside <markdown></markdown> tags from text.\n",
    "    If these tags don't exist, returns the entire text.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The text to parse\n",
    "        \n",
    "    Returns:\n",
    "        str: The extracted markdown content or the original text\n",
    "    \"\"\"\n",
    "    # Look for content between <markdown> and </markdown> tags\n",
    "    \n",
    "    markdown_pattern = re.compile(r'<markdown>(.*?)</markdown>', re.DOTALL)\n",
    "    match = markdown_pattern.search(text)\n",
    "    \n",
    "    if match:\n",
    "        # Return only the content inside the tags\n",
    "        return match.group(1)\n",
    "    else:\n",
    "        # If tags don't exist, return the entire text\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_out_markdown_tildas(text):\n",
    "    \"\"\"\n",
    "    Finds all occurrences of ```markdown ... ``` in the text and extracts\n",
    "    only the content inside, removing the markdown code block delimiters.\n",
    "    Also preserves any text outside of these blocks.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The text to process\n",
    "        \n",
    "    Returns:\n",
    "        str: The text with all markdown code blocks replaced with just their content\n",
    "    \"\"\"\n",
    "    # Pattern to match ```markdown ... ``` blocks\n",
    "    # The (?s) makes the dot match newlines as well\n",
    "    pattern = r\"```markdown\\s*((?:.|\\n)*?)```\"\n",
    "    \n",
    "    # Replace each match with just the content inside\n",
    "    result = re.sub(pattern, r\"\\1\", text)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_page_to_image(pdf_path, page_index, output_dir=None, dpi=300):\n",
    "    \"\"\"\n",
    "    Convert a specific page of a PDF to an image and optionally save it.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path: Path to the PDF file\n",
    "        page_index: Zero-based index of the page to convert\n",
    "        output_dir: Directory to save the image (if None, image is not saved)\n",
    "        dpi: Resolution of the output image (default: 300)\n",
    "        \n",
    "    Returns:\n",
    "        PIL Image object of the specified PDF page and path to saved image (if saved)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Open the PDF file\n",
    "        pdf_document = fitz.open(pdf_path)\n",
    "        \n",
    "        # Check if page_index is valid\n",
    "        if page_index < 0 or page_index >= len(pdf_document):\n",
    "            raise ValueError(f\"Page index {page_index} is out of range. PDF has {len(pdf_document)} pages.\")\n",
    "        \n",
    "        # Get the specified page\n",
    "        page = pdf_document.load_page(page_index)\n",
    "        \n",
    "        # Convert page to a pixmap (image)\n",
    "        pix = page.get_pixmap(matrix=fitz.Matrix(dpi/72, dpi/72))\n",
    "        \n",
    "        # Convert pixmap to PIL Image\n",
    "        img_data = pix.tobytes(\"png\")\n",
    "        img = Image.open(io.BytesIO(img_data))\n",
    "        \n",
    "        # Close the PDF document\n",
    "        pdf_document.close()\n",
    "        \n",
    "        # Save the image if output_dir is provided\n",
    "        saved_path = None\n",
    "        if output_dir is not None:\n",
    "            # Create output directory if it doesn't exist\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "            \n",
    "            # Get base name of the PDF file without extension\n",
    "            base_name = os.path.splitext(os.path.basename(pdf_path))[0]\n",
    "            \n",
    "            # Create output filename\n",
    "            output_filename = f\"{base_name}_{page_index}.png\"\n",
    "            output_path = os.path.join(output_dir, output_filename)\n",
    "            \n",
    "            # Save the image\n",
    "            img.save(output_path)\n",
    "            saved_path = output_path\n",
    "            print(f\"Saved page {page_index} to {output_path}\")\n",
    "        \n",
    "        if saved_path is None:\n",
    "            return img\n",
    "        else:\n",
    "            return img, saved_path\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error converting PDF page to image: {e}\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_bbs(response_text, pdf_path):\n",
    "    \"\"\"\n",
    "    Parse the bounding box information from the Gemini response text.\n",
    "\n",
    "    Args:\n",
    "        response_text (str): The text response containing bounding box tags.\n",
    "        pdf_path (str): The path to the PDF file.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries, each containing bounding box, page number,\n",
    "              and PDF path. Returns an empty list if no bounding boxes are found\n",
    "              or if there's an error parsing.\n",
    "    \"\"\"\n",
    "    bounding_boxes_info = []\n",
    "    # Regex to find the bounding box and page number within the tags\n",
    "    # It looks for <bounding_box>([list], page_number)</bounding_box>\n",
    "    # and captures the list part and the page_number part.\n",
    "    pattern = r\"<bounding_box>\\(\\[([\\d,\\s]+)\\],\\s*(\\d+)\\)</bounding_box>\"\n",
    "\n",
    "    matches = re.findall(pattern, response_text)\n",
    "\n",
    "    for match in matches:\n",
    "        try:\n",
    "            # Extract the coordinates string and the page number string\n",
    "            coords_str, page_num_str = match\n",
    "\n",
    "            # Convert coordinate string to a list of integers\n",
    "            # Removes spaces and splits by comma\n",
    "            coords = [int(c.strip()) for c in coords_str.split(',')]\n",
    "\n",
    "            # Convert page number string to integer\n",
    "            page_num = int(page_num_str)\n",
    "\n",
    "            # Append the structured data to the list\n",
    "            bounding_boxes_info.append({\n",
    "                \"bounding_box\": coords,\n",
    "                \"page\": page_num,\n",
    "                \"pdf\": pdf_path\n",
    "            })\n",
    "        except ValueError as e:\n",
    "            print(f\"Error parsing bounding box data: {match}. Error: {e}\")\n",
    "            # Skip this entry if there's an error converting numbers\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            print(f\"An unexpected error occurred while parsing: {match}. Error: {e}\")\n",
    "            continue\n",
    "\n",
    "    return bounding_boxes_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_images(image_path_or_data, bounding_boxes):\n",
    "    \"\"\"\n",
    "    Extract regions from an image based on bounding boxes and save them as PNG files.\n",
    "    \n",
    "    Args:\n",
    "        original_image: PIL Image object of the original document\n",
    "        bounding_boxes: List of bounding boxes in [ymin, xmin, ymax, xmax] format\n",
    "        output_dir: Directory to save the extracted images\n",
    "    \n",
    "    Returns:\n",
    "        List of paths to the saved images\n",
    "    \"\"\"\n",
    "    if isinstance(image_path_or_data, str):\n",
    "        try:\n",
    "            original_image = Image.open(image_path_or_data)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            return None\n",
    "    elif isinstance(image_path_or_data, Image.Image):\n",
    "        original_image = image_path_or_data\n",
    "    else:\n",
    "        raise ValueError(\"Invalid input type. Expected a path or PIL Image object.\")\n",
    "    \n",
    "    cropped_images = []\n",
    "    \n",
    "    # Process each bounding box\n",
    "    for i, box in enumerate(bounding_boxes):\n",
    "        # Extract coordinates\n",
    "        ymin, xmin, ymax, xmax = box['bounding_box']\n",
    "        \n",
    "        width, height = original_image.size\n",
    "        xmin = int((xmin/1000) * width)\n",
    "        xmax = int((xmax/1000) * width)\n",
    "        ymin = int((ymin/1000) * height)\n",
    "        ymax = int((ymax/1000) * height)\n",
    "        \n",
    "        # Crop the image\n",
    "        cropped_img = original_image.crop((xmin, ymin, xmax, ymax))\n",
    "        cropped_images.append(cropped_img)\n",
    "    \n",
    "    return cropped_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_all_images(file_path, bbs):\n",
    "    pdf_docu_length = fitz.open(file_path).page_count\n",
    "    pdf_images = [pdf_page_to_image(file_path, i) for i in range(pdf_docu_length)]\n",
    "    all_extracted_images = []\n",
    "    for bb in bbs:\n",
    "        page_num = bb['page']\n",
    "        corresponding_image = pdf_images[page_num - 1]\n",
    "        extracted_images = extract_images(corresponding_image, [bb])\n",
    "        all_extracted_images.append((extracted_images[0], bb['bounding_box'], bb['page']))\n",
    "        \n",
    "    return all_extracted_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_extracted_images(all_extracted_images, pdf_path):\n",
    "    \"\"\"\n",
    "    Saves extracted images to a folder named after the PDF file.\n",
    "    \n",
    "    Args:\n",
    "        all_extracted_images: List of tuples (image, bounding_box, page_number)\n",
    "        pdf_path: Path to the original PDF file\n",
    "    \n",
    "    Returns:\n",
    "        List of dictionaries with the original bounding box info plus the path to the saved image\n",
    "    \"\"\"\n",
    "    # Create folder name from PDF filename (replace spaces with underscores, remove extension)\n",
    "    pdf_name = os.path.basename(pdf_path)\n",
    "    folder_name = os.path.splitext(pdf_name)[0].replace(\" \", \"_\")\n",
    "    \n",
    "    # Create the folder if it doesn't exist\n",
    "    os.makedirs(folder_name, exist_ok=True)\n",
    "    \n",
    "    # Dictionary to keep track of image counts per page\n",
    "    image_counts = {}\n",
    "    \n",
    "    # List to store updated info with image paths\n",
    "    saved_images_info = []\n",
    "    \n",
    "    # Process each extracted image\n",
    "    for i, (image, bbox, page_num) in enumerate(all_extracted_images):\n",
    "        # Initialize counter for this page if not already done\n",
    "        if page_num not in image_counts:\n",
    "            image_counts[page_num] = 1\n",
    "        else:\n",
    "            image_counts[page_num] += 1\n",
    "        \n",
    "        # Create filename: PageX_ImageY.png\n",
    "        image_filename = f\"Page{page_num}_Image{image_counts[page_num]}.png\"\n",
    "        image_path = os.path.join(folder_name, image_filename)\n",
    "        \n",
    "        # Save the image\n",
    "        image.save(image_path)\n",
    "        print(f\"Saved image to {image_path}\")\n",
    "        \n",
    "        # Create info dictionary with original data plus the path\n",
    "        image_info = {\n",
    "            \"bounding_box\": bbox,\n",
    "            \"page\": page_num,\n",
    "            \"pdf\": pdf_path,\n",
    "            \"corresponding_image\": image_path\n",
    "        }\n",
    "        \n",
    "        saved_images_info.append(image_info)\n",
    "    \n",
    "    return saved_images_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_into_markdown(markdown_text, bounding_boxes_info):\n",
    "    \"\"\"\n",
    "    Replaces bounding box placeholders in markdown text with Markdown image links.\n",
    "\n",
    "    Args:\n",
    "        markdown_text (str): The original markdown text with <bounding_box> tags.\n",
    "        bounding_boxes_info (list): List of dictionaries, modified by\n",
    "                                     extract_and_save_images to include\n",
    "                                     'corresponding_image' paths.\n",
    "\n",
    "    Returns:\n",
    "        str: The markdown text with image links inserted.\n",
    "    \"\"\"\n",
    "    # Use an iterator to fetch the corresponding info for each match sequentially\n",
    "    info_iterator = iter(bounding_boxes_info)\n",
    "\n",
    "    # Define the replacement function for re.sub\n",
    "    def replacer(match):\n",
    "        try:\n",
    "            # Get the next bounding box info dictionary from the iterator\n",
    "            info = next(info_iterator)\n",
    "            image_path = info.get(\"corresponding_image\") # Use .get for safety\n",
    "\n",
    "            if image_path:\n",
    "                # Ensure path uses forward slashes for Markdown/HTML compatibility\n",
    "                image_path_md = pathlib.Path(image_path).as_posix()\n",
    "                # Extract filename without extension for alt text (e.g., \"Page0_Image1\")\n",
    "                alt_text = os.path.splitext(os.path.basename(image_path))[0]\n",
    "                # Return the Markdown image syntax\n",
    "                return f\"![{alt_text}]({image_path_md})\"\n",
    "            else:\n",
    "                # If corresponding_image is missing (e.g., extraction failed)\n",
    "                print(f\"Warning: Missing 'corresponding_image' for a matched bounding box tag. Original tag kept: {match.group(0)}\")\n",
    "                return match.group(0) # Return the full matched tag\n",
    "        except StopIteration:\n",
    "            # If there are more tags in text than info items\n",
    "            print(f\"Warning: More bounding box tags found in text than info items provided. Original tag kept: {match.group(0)}\")\n",
    "            return match.group(0)\n",
    "        except Exception as e:\n",
    "            print(f\"Error during replacement for tag {match.group(0)}: {e}. Original tag kept.\")\n",
    "            return match.group(0)\n",
    "\n",
    "    # The same pattern used in parse_bbs to find the tags\n",
    "    pattern = r\"<bounding_box>\\(\\[[\\d,\\s]+\\]\\,\\s*\\d+\\)</bounding_box>\"\n",
    "\n",
    "    # Perform the substitution using the replacer function\n",
    "    modified_markdown = re.sub(pattern, replacer, markdown_text)\n",
    "\n",
    "    return modified_markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved image to Mixture_of_Modular_Experts_Distilling_Knowledge_from_a_Multilingual_Teacher_into_Specialized_Modular_Language_Models\\Page4_Image1.png\n",
      "Saved image to Mixture_of_Modular_Experts_Distilling_Knowledge_from_a_Multilingual_Teacher_into_Specialized_Modular_Language_Models\\Page5_Image1.png\n",
      "Saved image to Mixture_of_Modular_Experts_Distilling_Knowledge_from_a_Multilingual_Teacher_into_Specialized_Modular_Language_Models\\Page6_Image1.png\n",
      "Saved image to Mixture_of_Modular_Experts_Distilling_Knowledge_from_a_Multilingual_Teacher_into_Specialized_Modular_Language_Models\\Page6_Image2.png\n",
      "Saved image to Mixture_of_Modular_Experts_Distilling_Knowledge_from_a_Multilingual_Teacher_into_Specialized_Modular_Language_Models\\Page7_Image1.png\n",
      "Saved image to Mixture_of_Modular_Experts_Distilling_Knowledge_from_a_Multilingual_Teacher_into_Specialized_Modular_Language_Models\\Page7_Image2.png\n",
      "Saved image to Mixture_of_Modular_Experts_Distilling_Knowledge_from_a_Multilingual_Teacher_into_Specialized_Modular_Language_Models\\Page8_Image1.png\n",
      "Saved image to Mixture_of_Modular_Experts_Distilling_Knowledge_from_a_Multilingual_Teacher_into_Specialized_Modular_Language_Models\\Page9_Image1.png\n",
      "Saved image to Mixture_of_Modular_Experts_Distilling_Knowledge_from_a_Multilingual_Teacher_into_Specialized_Modular_Language_Models\\Page10_Image1.png\n",
      "Saved image to Mixture_of_Modular_Experts_Distilling_Knowledge_from_a_Multilingual_Teacher_into_Specialized_Modular_Language_Models\\Page12_Image1.png\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "WindowsPath('pdf1_promptv2_images.md')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbs = parse_bbs(response.text, pdf1)\n",
    "images = extract_all_images(pdf1, bbs)\n",
    "saved_images_info = save_extracted_images(images, pdf1)\n",
    "md = insert_into_markdown(response.text, saved_images_info)\n",
    "save_to_markdown(parse_out_markdown_tildas(md), \"pdf1_promptv2_images.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
